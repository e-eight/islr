<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-05-30 Sat 13:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Linear Regression</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="static/css/simple.css"/>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="sitemap.html"> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content">
<h1 class="title">Linear Regression</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org8e046a8">Notes</a>
<ul>
<li><a href="#orgb0cbd29">Accuracy</a></li>
<li><a href="#org35d57c7">Outliers &amp; High Leverage points</a></li>
</ul>
</li>
<li><a href="#orgbffa4d1">Exercises</a>
<ul>
<li><a href="#orgef2e2f3">Question 1</a></li>
<li><a href="#orgef41b10">Question 2</a></li>
<li><a href="#orgaea3cf8">Question 3</a></li>
<li><a href="#org62784b7">Question 4</a></li>
<li><a href="#org4bbecec">Question 5</a></li>
<li><a href="#org5f006c0">Question 6</a></li>
<li><a href="#org4add57b">Question 7</a></li>
<li><a href="#org27a90af">Question 8</a>
<ul>
<li><a href="#org8f9e163">Simple linear regression on <code>Auto</code> data set</a></li>
<li><a href="#org550c2ad">Least squares plot</a></li>
<li><a href="#orgb4551c0">Diagnostic plots</a></li>
</ul>
</li>
<li><a href="#org36d602c">Question 9</a>
<ul>
<li><a href="#org3a9ad3d">Scatter plot matrix of <code>Auto</code> data set</a></li>
<li><a href="#org61e1d86">Correlation matrix</a></li>
<li><a href="#org3925d51">Multiple linear regression with <code>Auto</code> data set</a></li>
<li><a href="#org495426b">Diagnostic plots</a></li>
<li><a href="#org72f6214">Interaction effects</a></li>
<li><a href="#org1f3ccc0">Models with variable transformations</a></li>
</ul>
</li>
<li><a href="#org0878758">Question 10</a>
<ul>
<li><a href="#org586eb8d">Multiple regression with <code>Carseats</code> data set</a></li>
<li><a href="#org5ec70bf">Interpretation of coefficient of predictors</a></li>
<li><a href="#orgd7b053c">Linear model equation</a></li>
<li><a href="#org392ea7c">Null hypothesis rejection</a></li>
<li><a href="#orgc75a2b3">Smaller multiple linear model for <code>Carseats</code> sales</a></li>
<li><a href="#orgc2abcaa">Confidence intervals of fitted parameters</a></li>
<li><a href="#org16ec8fc">Outliers and leverages</a></li>
</ul>
</li>
<li><a href="#org6b1e95a">Question 11</a>
<ul>
<li><a href="#org4b36e9c">Simple linear regression with synthetic data</a></li>
<li><a href="#org13982fb">Inverse simple linear relation with synthetic data</a></li>
<li><a href="#org0005b62">Relation between the two regressions</a></li>
<li><a href="#orgfb9ddec">t-statistic for first model</a></li>
<li><a href="#orge972263">t-statistic for second model</a></li>
<li><a href="#orgf76947f">t-statistic for models with intercept</a></li>
</ul>
</li>
<li><a href="#org56e7d6c">Question 12</a>
<ul>
<li><a href="#org51cb211">Equal regression coefficients</a></li>
<li><a href="#orgcd46063">Different coefficient estimates - numerical</a></li>
<li><a href="#org14b71fd">Same coefficient estimates - numerical</a></li>
</ul>
</li>
<li><a href="#org1d7b4c8">Question 13</a>
<ul>
<li><a href="#org4cb4e2a">Feature vector</a></li>
<li><a href="#org4e47f56">Error vector</a></li>
<li><a href="#org3b09a3a">Response vector</a></li>
<li><a href="#org50682f4">Scatter plot</a></li>
<li><a href="#orgd2b17d0">Least squares fit</a></li>
<li><a href="#orga4cbe44">Polynomial regression</a></li>
<li><a href="#orgb84e7ca">Least squares fit with less noise</a></li>
<li><a href="#org1fbef9d">Least squares fit with more noise</a></li>
<li><a href="#org6267844">Confidence intervals of the three models</a></li>
</ul>
</li>
<li><a href="#org9dd5f43">Question 14</a>
<ul>
<li><a href="#orga8e527f">Multiple linear model with collinearity</a></li>
<li><a href="#org1ffa385">Correlation scatter plot</a></li>
<li><a href="#org5b8872f">Least squares fit with <code>x1</code> and <code>x2</code></a></li>
<li><a href="#org3e3abee">Least squares fit with <code>x1</code> only</a></li>
<li><a href="#org51964ec">Least squares fit with <code>x2</code> only</a></li>
<li><a href="#orgcbd16b4">Contradiction of models</a></li>
<li><a href="#org8a053d2">Additional data</a></li>
</ul>
</li>
<li><a href="#org2062a94">Question 15</a>
<ul>
<li><a href="#org22249f6">Predict per capita crime rate with the <code>Boston</code> data set</a></li>
<li><a href="#org20e3b7c">Multiple regression with <code>Boston</code> data set</a></li>
<li><a href="#orgb658cac">Comparison plot</a></li>
<li><a href="#org93bccb1">Evidence of non-linear associations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org8e046a8" class="outline-2">
<h2 id="org8e046a8">Notes</h2>
<div class="outline-text-2" id="text-org8e046a8">
</div>
<div id="outline-container-orgb0cbd29" class="outline-3">
<h3 id="orgb0cbd29">Accuracy</h3>
<div class="outline-text-3" id="text-orgb0cbd29">
<ul class="org-ul">
<li>Residual sum of squares (RSS) = \(\sum_{i=1}^n (y_i - \hat{y}_i)^2\).</li>
<li>Residual standard error (RSE) = \(\sqrt{\frac{1}{n-2}\text{RSS}}\).</li>
<li>\(R^2\) = 1 - \(\frac{\text{RSS}}{\text{TSS}}\).</li>
<li>Total sum of squares (TSS) =  \(\sum_{i=1}^n (y_i - \bar{y}_i)^2\).</li>
</ul>
</div>
</div>

<div id="outline-container-org35d57c7" class="outline-3">
<h3 id="org35d57c7">Outliers &amp; High Leverage points</h3>
<div class="outline-text-3" id="text-org35d57c7">
<ul class="org-ul">
<li>Outlier is an observation for which the predicted response is far from the
recorded response.</li>
<li>High leverage point is an observation for which the recorded predictors are
unusual.</li>
<li>High leverage points usually have more influence on the model than outliers.</li>
<li>The studentized (standardized) residuals are typically expected to be between
-3 and 3. If they lie outside then that indicates that there might be outliers
in the data.</li>
<li>The average leverage is \((p + 1) / n\). If the leverage of some observation
is significantly higher than the average leverage then that is a high leverage
point. (Look in to "Cook's distance".)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbffa4d1" class="outline-2">
<h2 id="orgbffa4d1">Exercises</h2>
<div class="outline-text-2" id="text-orgbffa4d1">
</div>
<div id="outline-container-orgef2e2f3" class="outline-3">
<h3 id="orgef2e2f3">Question 1</h3>
<div class="outline-text-3" id="text-orgef2e2f3">
<p>
Since there are three p-values in Table 3.4, there are three null hypotheses:
</p>
<ul class="org-ul">
<li>H<sub>0</sub><sup>(1)</sup>: <code>sales</code> does not depend on <code>TV</code></li>
<li>H<sub>0</sub><sup>(2)</sup>: <code>sales</code> does not depend on <code>radio</code></li>
<li>H<sub>0</sub><sup>(3)</sup>: <code>sales</code> does not depend on <code>newspaper</code></li>
</ul>
<p>
The p-values corresponding to the first two null hypotheses are very small, less
than 10<sup>-4</sup>, whereas the p-value for the last one is close to 1. This means that
the probability of seeing the observed <code>sales</code> numbers if H<sub>0</sub><sup>(1)</sup> or H<sub>0</sub><sup>(2)</sup> is true
is almost zero, but the probability of seeing the observed <code>sales</code> numbers if
H<sub>0</sub><sup>(3)</sup> is true is almost 1. In other words <code>sales</code> depends on <code>TV</code> and <code>radio</code>
but not on <code>newspaper</code>.
</p>
</div>
</div>

<div id="outline-container-orgef41b10" class="outline-3">
<h3 id="orgef41b10">Question 2</h3>
<div class="outline-text-3" id="text-orgef41b10">
<p>
A KNN classifier classifies an observation is to the class with the <b>majority</b>
of nearest neighbors. In the KNN regression method the response is estimated as
the <b>average</b> of all the training responses of the nearest neighbors.
</p>
</div>
</div>

<div id="outline-container-orgaea3cf8" class="outline-3">
<h3 id="orgaea3cf8">Question 3</h3>
<div class="outline-text-3" id="text-orgaea3cf8">
<ol id="al" class="org-ol">
<li>Assuming X<sub>4</sub> = X<sub>1</sub> X<sub>2</sub> and X<sub>5</sub> = X<sub>1</sub> X<sub>3</sub>, the linear fit model is as follows: Y =
50 + 20 X<sub>1</sub> + 0.07 X<sub>2</sub> + 35 X<sub>3</sub> + 0.01 X<sub>1</sub> X<sub>2</sub> - 10 X<sub>3</sub> X<sub>5</sub>. Since X<sub>3</sub> (Gender) is 1
for females and 0 for males, this fit essentially gives two models based on
the gender: Y = 85 + 10 X<sub>1</sub> + 0.07 X<sub>2</sub> + 0.01 X<sub>1</sub> X<sub>2</sub> for females, and Y = 50 +
20 X<sub>1</sub> + 0.07 X<sub>2</sub> + 0.01 X<sub>1</sub> X<sub>2</sub> for males. If X<sub>1</sub> (GPA), and X<sub>2</sub> (IQ), are fixed
for the two genders then the starting salary for males will be more on
average than the starting salary of females if X<sub>1</sub> &gt; 35 / 10 = 3.5; i.e. given
the GPA is high enough, on average males will have a higher starting salary
than females (iii).</li>
<li>If IQ is 110 and GPA is 4.0 then a female is predicted to have a starting
salary of $ 137,100.</li>
<li>False. The scale for IQ is much larger than that of GPA or Gender. Hence the
coefficients related to any IQ based predictor are expected to be small.
(Possibly this is why it is often recommended to scale the predictors, so
they are all on an equal footing and it is easier to figure out which ones
are important.)</li>
</ol>
</div>
</div>

<div id="outline-container-org62784b7" class="outline-3">
<h3 id="org62784b7">Question 4</h3>
<div class="outline-text-3" id="text-org62784b7">
<ol id="al" class="org-ol">
<li>Even though the true relationship is linear, we need to take into account the
irreducible error \(Ïµ\). The cubic model is more flexible than the linear
model and therefore will be able to better fit to the irreducible error. I
expect the cubic model to have lower <b>training</b> RSS than the linear model.</li>
<li>The linear model will have a lower <b>test</b> RSS than the cubic model since it
matches the true relationship. The cubic model would have overfitted to the
irreducible error in the training data and therefore perform worse on the
test data.</li>
<li>Irrespective of the true relationship I would expect the cubic model to have
lower training RSS than the linear model because it is more flexible than the
linear model and thus fits the training data better.</li>
<li>Since we do not the true relationship between X and Y, it is difficult to say
which of the two models will have lower test RSS without more information.</li>
</ol>
</div>
</div>

<div id="outline-container-org4bbecec" class="outline-3">
<h3 id="org4bbecec">Question 5</h3>
<div class="outline-text-3" id="text-org4bbecec">
<p>
We just have to substitute equation 3.38 in to the equation for the \(i\)th
fitted value and rearrange the equation:
</p>
\begin{align}
  a_{i'} = \frac{x_i x_{i'}}{\sum_{k=1}^n x_k^2}.
\end{align}
</div>
</div>

<div id="outline-container-org5f006c0" class="outline-3">
<h3 id="org5f006c0">Question 6</h3>
<div class="outline-text-3" id="text-org5f006c0">
<p>
The equation for the least squares line is \(y = \hat{\beta}_0 + \hat{\beta}_1
x\). From equation 3.4 we have \(\hat{\beta}_0 = \bar{y} - \hat{\beta}_1
\bar{x}\). Then putting \(x = \bar{x}\) in the least squares line equation we
see that \(y = \bar{y}\). Thus the least squares line always passes through
\((\bar{x}, \bar{y})\).
</p>
</div>
</div>

<div id="outline-container-org4add57b" class="outline-3">
<h3 id="org4add57b">Question 7</h3>
<div class="outline-text-3" id="text-org4add57b">
<p>
Assumption: \(\bar{x} = \bar{y} = 0\). From equation 3.4 we have \(\hat{y}_i =
\hat{\beta}_1 x_i\), where
</p>
\begin{align}
  \hat{\beta}_1 = \frac{\sum_{i=1}^n x_i y_i}{\sum_{i=1}^n x_i^2}.
\end{align}
<p>
And from equation 3.17 we have
</p>
\begin{align}
  R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n y_i^2}.
\end{align}
<p>
Expanding the square in the numerator of the above fraction and substituting
\(\hat{y}_i\) with \(\hat{\beta}_1 x_i\) we get
</p>
\begin{align}
  R^2 = \frac{\hat{\beta}_1(2 \sum_{i=1}^n x_iy_i - \hat{\beta}_1 \sum_{i=1}^n x_i^2)}{\sum_{i=1}^n y_i^2}.
\end{align}
<p>
Now if we simply substitute the above form of \(\hat{\beta}_1\) in to this
equation we will see that \(R^2 = \mathrm{Cor}(X, Y)^2\), where \(\mathrm{Cor}(X,
Y)\) is given by equation 3.18.
</p>
</div>
</div>

<div id="outline-container-org27a90af" class="outline-3">
<h3 id="org27a90af">Question 8</h3>
<div class="outline-text-3" id="text-org27a90af">
</div>
<div id="outline-container-org8f9e163" class="outline-4">
<h4 id="org8f9e163">Simple linear regression on <code>Auto</code> data set</h4>
<div class="outline-text-4" id="text-org8f9e163">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> pandas <span style="color: #859900; font-weight: bold;">as</span> pd
<span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #859900; font-weight: bold;">from</span> tabulate <span style="color: #859900; font-weight: bold;">import</span> tabulate

<span style="color: #268bd2;">auto</span> = pd.read_csv<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"data/Auto.csv"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>tabulate<span style="color: #d33682;">(</span>auto.head<span style="color: #859900;">()</span>, auto.columns, tablefmt=<span style="color: #2aa198;">"orgtbl"</span><span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
|    |   mpg |   cylinders |   displacement |   horsepower |   weight |   acceleration |   year |   origin | name                      |
|----+-------+-------------+----------------+--------------+----------+----------------+--------+----------+---------------------------|
|  0 |    18 |           8 |            307 |          130 |     3504 |           12   |     70 |        1 | chevrolet chevelle malibu |
|  1 |    15 |           8 |            350 |          165 |     3693 |           11.5 |     70 |        1 | buick skylark 320         |
|  2 |    18 |           8 |            318 |          150 |     3436 |           11   |     70 |        1 | plymouth satellite        |
|  3 |    16 |           8 |            304 |          150 |     3433 |           12   |     70 |        1 | amc rebel sst             |
|  4 |    17 |           8 |            302 |          140 |     3449 |           10.5 |     70 |        1 | ford torino               |

</pre>

<p>
Recall from the last chapter <code>horsepower</code> has some missing values and needs to
be converted to a numeric form before we can use this for linear regression.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">auto.drop<span style="color: #268bd2;">(</span>auto<span style="color: #d33682;">[</span>auto.horsepower == <span style="color: #2aa198;">"?"</span><span style="color: #d33682;">]</span>.index, inplace=<span style="color: #6c71c4; font-weight: bold;">True</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">auto</span><span style="color: #268bd2;">[</span><span style="color: #2aa198;">"horsepower"</span><span style="color: #268bd2;">]</span> = pd.to_numeric<span style="color: #268bd2;">(</span>auto<span style="color: #d33682;">[</span><span style="color: #2aa198;">"horsepower"</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<p>
For simple linear regression we can use the ordinary least squares model from
<code>statsmodels</code> or the linear regression model from <code>scikit-learn</code>. In this
question we are asked to print the summary of the fitted model. <code>scikit-learn</code>
has no method for generating a summary, but <code>statsmodels</code> does.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.formula.api <span style="color: #859900; font-weight: bold;">as</span> smf

<span style="color: #268bd2;">model</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"mpg~horsepower"</span>, data=auto<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.606
Model:                            OLS   Adj. R-squared:                  0.605
Method:                 Least Squares   F-statistic:                     599.7
Date:                Mon, 25 May 2020   Prob (F-statistic):           7.03e-81
Time:                        17:19:34   Log-Likelihood:                -1178.7
No. Observations:                 392   AIC:                             2361.
Df Residuals:                     390   BIC:                             2369.
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     39.9359      0.717     55.660      0.000      38.525      41.347
horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145
==============================================================================
Omnibus:                       16.432   Durbin-Watson:                   0.920
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305
Skew:                           0.492   Prob(JB):                     0.000175
Kurtosis:                       3.299   Cond. No.                         322.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<ol id="rm" class="org-ol">
<li>The F-statistic is much larger than 1, and the p-value (<code>P&gt;|t|</code> in the table)
is zero. This indicates that there is a relationship between <code>mpg</code> and
<code>horsepower</code>.</li>
<li>The \(R^2\) value of 0.606 indicates that this relationship explains around
61% of the <code>mpg</code> values.</li>
<li>The coefficient value corresponding to <code>horsepower</code> is negative. This
indicates that the relation between <code>mpg</code> and <code>horsepower</code> is negative.</li>
<li><div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">pred</span> = model.get_prediction<span style="color: #268bd2;">(</span>exog=<span style="color: #d33682; font-style: italic;">dict</span><span style="color: #d33682;">(</span>horsepower=98<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">pred_summary</span> = pred.summary_frame<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>tabulate<span style="color: #d33682;">(</span>pred_summary, pred_summary.columns, tablefmt=<span style="color: #2aa198;">"orgtbl"</span><span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
|    |    mean |   mean_se |   mean_ci_lower |   mean_ci_upper |   obs_ci_lower |   obs_ci_upper |
|----+---------+-----------+-----------------+-----------------+----------------+----------------|
|  0 | 24.4671 |  0.251262 |         23.9731 |         24.9611 |        14.8094 |        34.1248 |

</pre>

<p>
The predicted <code>mpg</code> for <code>horsepower</code> = 98 is 24.4671. The 95% confidence
interval is \([23.9731, 24.9611]\), and the 95% prediction interval is
\([14.8094, 34.1248]\). As mentioned in the text, the prediction interval
contains the confidence interval.
</p></li>
</ol>
</div>
</div>

<div id="outline-container-org550c2ad" class="outline-4">
<h4 id="org550c2ad">Least squares plot</h4>
<div class="outline-text-4" id="text-org550c2ad">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #859900; font-weight: bold;">as</span> plt
<span style="color: #859900; font-weight: bold;">import</span> seaborn <span style="color: #859900; font-weight: bold;">as</span> sns

sns.set_style<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"ticks"</span><span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">X</span> = auto<span style="color: #268bd2;">[</span><span style="color: #2aa198;">"horsepower"</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">Y</span> = auto<span style="color: #268bd2;">[</span><span style="color: #2aa198;">"mpg"</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">Ypred</span> = model.predict<span style="color: #268bd2;">(</span>X<span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
ax.plot<span style="color: #268bd2;">(</span>X, Y, <span style="color: #2aa198;">'o'</span>, label=<span style="color: #2aa198;">"Data"</span><span style="color: #268bd2;">)</span>
ax.plot<span style="color: #268bd2;">(</span>X, Ypred, <span style="color: #2aa198;">'r'</span>, label=<span style="color: #2aa198;">"Least Squares Regression"</span><span style="color: #268bd2;">)</span>
ax.legend<span style="color: #268bd2;">(</span>loc=<span style="color: #2aa198;">"best"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_auto_ls.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_auto_ls.png" alt="3_auto_ls.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgb4551c0" class="outline-4">
<h4 id="orgb4551c0">Diagnostic plots</h4>
<div class="outline-text-4" id="text-orgb4551c0">
<p>
The <code>R</code> command <code>plot</code> in this case gives four diagnostic plots:
</p>
<ul class="org-ul">
<li>Residuals vs Fitted</li>
<li>Normal Q-Q</li>
<li>Scale-Location</li>
<li>Residuals vs Leverage</li>
</ul>

<p>
The <b>Residuals vs Fitted</b> plot shows any non-linear pattern in the residuals,
and by extension in the data.
The <b>Normal Q-Q</b> plot shows if the residuals are normally distributed.
The <b>Scale-Location</b> plot shows if there is heteroscedasticity.
The <b>Residuals vs Leverage</b> plot shows if there are leverages in the data.
</p>

<p>
We will produce these plots using <code>statsmodels</code> and <code>seaborn</code>.
</p>
</div>

<ul class="org-ul">
<li><a id="orgce275c7"></a>Residuals vs Fitted plot<br />
<div class="outline-text-5" id="text-orgce275c7">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">fitted_vals</span> = model.fittedvalues

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">residplot</span> = sns.residplot<span style="color: #268bd2;">(</span>x=fitted_vals, y=<span style="color: #2aa198;">"mpg"</span>, data=auto,
                          lowess=<span style="color: #6c71c4; font-weight: bold;">True</span>,
                          line_kws=<span style="color: #d33682;">{</span><span style="color: #2aa198;">"color"</span>: <span style="color: #2aa198;">"red"</span><span style="color: #d33682;">}</span>,
                          ax=ax<span style="color: #268bd2;">)</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Fitted values"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Residuals"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_res_vs_fit.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_res_vs_fit.png" alt="3_res_vs_fit.png" />
</p>
</div>

<p>
This plot clearly shows that there is non-linearity in the data.
</p>
</div>
</li>

<li><a id="org580e2db"></a>Normal Q-Q plot<br />
<div class="outline-text-5" id="text-org580e2db">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.api <span style="color: #859900; font-weight: bold;">as</span> sm

<span style="color: #268bd2;">residuals</span> = model.resid

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">qqplot</span> = sm.qqplot<span style="color: #268bd2;">(</span>residuals, line=<span style="color: #2aa198;">'45'</span>, ax=ax, fit=<span style="color: #6c71c4; font-weight: bold;">True</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_auto_qq.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_auto_qq.png" alt="3_auto_qq.png" />
</p>
</div>

<p>
Though there are some points that are far from the \(45^\circ\) fitted line, most
of the points lie close to the line, indicating that the residuals are mostly
normally distributed.
</p>
</div>
</li>

<li><a id="org2f1fba4"></a>Scale-Location plot<br />
<div class="outline-text-5" id="text-org2f1fba4">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">normalized residuals and their square roots</span>
<span style="color: #268bd2;">norm_residuals</span> = model.get_influence<span style="color: #268bd2;">()</span>.resid_studentized_internal
<span style="color: #268bd2;">norm_residuals_abs_sqrt</span> = np.sqrt<span style="color: #268bd2;">(</span>np.<span style="color: #d33682; font-style: italic;">abs</span><span style="color: #d33682;">(</span>norm_residuals<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">slplot</span> = sns.regplot<span style="color: #268bd2;">(</span>fitted_vals, norm_residuals_abs_sqrt,
                     lowess=<span style="color: #6c71c4; font-weight: bold;">True</span>,
                     line_kws=<span style="color: #d33682;">{</span><span style="color: #2aa198;">"color"</span> : <span style="color: #2aa198;">"red"</span><span style="color: #d33682;">}</span>,
                     ax=ax<span style="color: #268bd2;">)</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Fitted values"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Sqrt of |Standardized Residuals|"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_auto_scale_loc.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_auto_scale_loc.png" alt="3_auto_scale_loc.png" />
</p>
</div>

<p>
This plot is similar to the first diagnostic plots, except now the quantity on
the y-axis is positive. This shows that homoscedasticity is not held, i.e. the
variance is not constant.
</p>
</div>
</li>

<li><a id="org8d1e12e"></a>Residuals vs Leverage plot<br />
<div class="outline-text-5" id="text-org8d1e12e">
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">rlplot</span> = sm.graphics.influence_plot<span style="color: #268bd2;">(</span>model, criterion=<span style="color: #2aa198;">"Cooks"</span>, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Leverage"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
ax.set_title<span style="color: #268bd2;">(</span><span style="color: #2aa198;">" "</span><span style="color: #268bd2;">)</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_auto_res_vs_lev.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_auto_res_vs_lev.png" alt="3_auto_res_vs_lev.png" />
</p>
</div>

<p>
We see that none of the points have a very high leverage.
</p>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-org36d602c" class="outline-3">
<h3 id="org36d602c">Question 9</h3>
<div class="outline-text-3" id="text-org36d602c">
</div>
<div id="outline-container-org3a9ad3d" class="outline-4">
<h4 id="org3a9ad3d">Scatter plot matrix of <code>Auto</code> data set</h4>
<div class="outline-text-4" id="text-org3a9ad3d">
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">spm</span> = sns.pairplot<span style="color: #268bd2;">(</span>auto, plot_kws = <span style="color: #d33682;">{</span><span style="color: #2aa198;">'s'</span>: 10<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>
spm.fig.set_size_inches<span style="color: #268bd2;">(</span>12, 12<span style="color: #268bd2;">)</span>
spm.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_auto_scatter.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_auto_scatter.png" alt="3_auto_scatter.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org61e1d86" class="outline-4">
<h4 id="org61e1d86">Correlation matrix</h4>
<div class="outline-text-4" id="text-org61e1d86">
<p>
I find heat maps to be better for visualizing correlation matrices than tables.
Since the correlation matrix is symmetric we can ignore either of the lower or
the upper triangles. We can also ignore the diagonal since it is always going to
be 1.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">corr_mat</span> = auto<span style="color: #268bd2;">[</span>auto.columns<span style="color: #d33682;">[</span>:-1<span style="color: #d33682;">]</span><span style="color: #268bd2;">]</span>.corr<span style="color: #268bd2;">()</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">Custom diverging color map.</span>
<span style="color: #268bd2;">cmap</span> = sns.diverging_palette<span style="color: #268bd2;">(</span>220, 10, sep=80, n=7<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">Mask for upper triangle.</span>
<span style="color: #268bd2;">mask</span> = np.triu<span style="color: #268bd2;">(</span>np.ones_like<span style="color: #d33682;">(</span>corr_mat, dtype=np.<span style="color: #d33682; font-style: italic;">bool</span><span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>

<span style="color: #859900; font-weight: bold;">with</span> sns.axes_style<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"white"</span><span style="color: #268bd2;">)</span>:
    sns.heatmap<span style="color: #268bd2;">(</span>corr_mat, mask=mask, cmap=cmap, annot=<span style="color: #6c71c4; font-weight: bold;">True</span>, robust=<span style="color: #6c71c4; font-weight: bold;">True</span>, ax=ax<span style="color: #268bd2;">)</span>

fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_auto_corr_heat.png"</span><span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_auto_corr_heat.png" alt="3_auto_corr_heat.png" />
</p>
</div>

<p>
We see that <code>mpg</code> has considerable negative correlations with <code>cylinders</code>,
<code>displacement</code>, <code>horsepower</code>, and <code>weight</code>. This matches what we saw in the
scatter plot matrix above. Similarly <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>
and <code>weight</code> are all correlated with each other.
</p>
</div>
</div>

<div id="outline-container-org3925d51" class="outline-4">
<h4 id="org3925d51">Multiple linear regression with <code>Auto</code> data set</h4>
<div class="outline-text-4" id="text-org3925d51">
<p>
We could do this with the <code>statsmodels.formula</code> API but that involves more
typing, so we will use the <code>statsmodels</code> API.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.api <span style="color: #859900; font-weight: bold;">as</span> sm

<span style="color: #268bd2;">Y</span> = auto<span style="color: #268bd2;">[</span><span style="color: #2aa198;">"mpg"</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">X</span> = auto<span style="color: #268bd2;">[</span>auto.columns<span style="color: #d33682;">[</span>1:-1<span style="color: #d33682;">]</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">X</span> = sm.add_constant<span style="color: #268bd2;">(</span>X<span style="color: #268bd2;">)</span> <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">For the intercept.</span>
<span style="color: #268bd2;">ml_model</span> = sm.OLS<span style="color: #268bd2;">(</span>Y, X<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.821
Model:                            OLS   Adj. R-squared:                  0.818
Method:                 Least Squares   F-statistic:                     252.4
Date:                Mon, 25 May 2020   Prob (F-statistic):          2.04e-139
Time:                        17:25:25   Log-Likelihood:                -1023.5
No. Observations:                 392   AIC:                             2063.
Df Residuals:                     384   BIC:                             2095.
Df Model:                           7
Covariance Type:            nonrobust
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const          -17.2184      4.644     -3.707      0.000     -26.350      -8.087
cylinders       -0.4934      0.323     -1.526      0.128      -1.129       0.142
displacement     0.0199      0.008      2.647      0.008       0.005       0.035
horsepower      -0.0170      0.014     -1.230      0.220      -0.044       0.010
weight          -0.0065      0.001     -9.929      0.000      -0.008      -0.005
acceleration     0.0806      0.099      0.815      0.415      -0.114       0.275
year             0.7508      0.051     14.729      0.000       0.651       0.851
origin           1.4261      0.278      5.127      0.000       0.879       1.973
==============================================================================
Omnibus:                       31.906   Durbin-Watson:                   1.309
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               53.100
Skew:                           0.529   Prob(JB):                     2.95e-12
Kurtosis:                       4.460   Cond. No.                     8.59e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 8.59e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<ol id="rm" class="org-ol">
<li>The large F-statistic indicates that we can ignore the null hypothesis, which
says that the response <code>mpg</code> does not depend on the predictors. The
probability that this data could be generated if the null hypothesis was true
is essentially zero (2.04E-139).</li>
<li>Looking at the p-values of the individual predictors we see that <code>weight</code>,
<code>year</code>, and <code>origin</code> have the most statistically significant relation with
<code>mpg</code>. We can also argue that <code>displacement</code> has a somewhat significant
relation with <code>mpg</code>. On the other hand <code>cylinders</code>, <code>horsepower</code>, and
<code>acceleration</code> do not have a significant statistical relationship. This is
not necessarily surprising. Given the correlation between <code>mpg</code>,
<code>displacement</code>, <code>cylinders</code> and <code>horsepower</code> I think one can argue that the
information in <code>cylinders</code> and <code>horsepower</code> is redundant.</li>
<li>The coefficient for the <code>year</code> variable suggests that every year the <code>mpg</code>
increases by <code>0.7508</code>, i.e. the cars become more fuel-efficient every year.</li>
</ol>
</div>
</div>

<div id="outline-container-org495426b" class="outline-4">
<h4 id="org495426b">Diagnostic plots</h4>
<div class="outline-text-4" id="text-org495426b">
<p>
We make the same diagnostic plots as the previous exercise.
</p>
</div>

<ul class="org-ul">
<li><a id="org5f54867"></a>Residuals vs Fitted plot<br />
<div class="outline-text-5" id="text-org5f54867">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">fitted_vals</span> = ml_model.fittedvalues

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">residplot</span> = sns.residplot<span style="color: #268bd2;">(</span>x=fitted_vals, y=<span style="color: #2aa198;">"mpg"</span>, data=auto,
                          lowess=<span style="color: #6c71c4; font-weight: bold;">True</span>,
                          line_kws=<span style="color: #d33682;">{</span><span style="color: #2aa198;">"color"</span>: <span style="color: #2aa198;">"red"</span><span style="color: #d33682;">}</span>,
                          ax=ax<span style="color: #268bd2;">)</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Fitted values"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Residuals"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_ml_res_vs_fit.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_ml_res_vs_fit.png" alt="3_ml_res_vs_fit.png" />
</p>
</div>

<p>
This plot clearly shows that there is non-linearity in the data.
</p>
</div>
</li>

<li><a id="org8c31417"></a>Normal Q-Q plot<br />
<div class="outline-text-5" id="text-org8c31417">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.api <span style="color: #859900; font-weight: bold;">as</span> sm

<span style="color: #268bd2;">residuals</span> = ml_model.resid

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">qqplot</span> = sm.qqplot<span style="color: #268bd2;">(</span>residuals, line=<span style="color: #2aa198;">'45'</span>, ax=ax, fit=<span style="color: #6c71c4; font-weight: bold;">True</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_ml_auto_qq.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_ml_auto_qq.png" alt="3_ml_auto_qq.png" />
</p>
</div>

<p>
Though there are some points that are far from the \(45^\circ\) fitted line, most
of the points lie close to the line, indicating that the residuals are mostly
normally distributed.
</p>
</div>
</li>

<li><a id="org89acfd1"></a>Scale-Location plot<br />
<div class="outline-text-5" id="text-org89acfd1">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">normalized residuals and their square roots</span>
<span style="color: #268bd2;">norm_residuals</span> = ml_model.get_influence<span style="color: #268bd2;">()</span>.resid_studentized_internal
<span style="color: #268bd2;">norm_residuals_abs_sqrt</span> = np.sqrt<span style="color: #268bd2;">(</span>np.<span style="color: #d33682; font-style: italic;">abs</span><span style="color: #d33682;">(</span>norm_residuals<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">slplot</span> = sns.regplot<span style="color: #268bd2;">(</span>fitted_vals, norm_residuals_abs_sqrt,
                     lowess=<span style="color: #6c71c4; font-weight: bold;">True</span>,
                     line_kws=<span style="color: #d33682;">{</span><span style="color: #2aa198;">"color"</span> : <span style="color: #2aa198;">"red"</span><span style="color: #d33682;">}</span>,
                     ax=ax<span style="color: #268bd2;">)</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Fitted values"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Sqrt of |Standardized Residuals|"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_ml_auto_scale_loc.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_ml_auto_scale_loc.png" alt="3_ml_auto_scale_loc.png" />
</p>
</div>

<p>
The variance in the standardized residuals is less as compared to the single
regression plot, but there is still quite a bit of variance, which means
homoscedasticity is not held.
</p>
</div>
</li>

<li><a id="orga439d17"></a>Residuals vs Leverage plot<br />
<div class="outline-text-5" id="text-orga439d17">
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">rlplot</span> = sm.graphics.influence_plot<span style="color: #268bd2;">(</span>ml_model, criterion=<span style="color: #2aa198;">"Cooks"</span>, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Leverage"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
ax.set_title<span style="color: #268bd2;">(</span><span style="color: #2aa198;">" "</span><span style="color: #268bd2;">)</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3_ml_auto_res_vs_lev.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3_ml_auto_res_vs_lev.png" alt="3_ml_auto_res_vs_lev.png" />
</p>
</div>

<p>
Point 13 has a high leverage but not a very high residual.
</p>
</div>
</li>
</ul>
</div>

<div id="outline-container-org72f6214" class="outline-4">
<h4 id="org72f6214">Interaction effects</h4>
<div class="outline-text-4" id="text-org72f6214">
<p>
We go back to the <code>statsmodels.formula</code> API. Additionally I will drop
<code>cylinders</code>, <code>horsepower</code>, and <code>acceleration</code> from the model. I will try the
following interaction terms:
</p>
<ul class="org-ul">
<li><code>year : origin</code> : this will add a new predictor which is a product of <code>year</code>
and <code>origin</code>, but will not include <code>year</code> and <code>origin</code> separately,</li>
<li><code>year * origin</code> : this will add the product of <code>year</code> and <code>origin</code>, but also
include <code>year</code> and <code>origin</code> separately,</li>
<li><code>year * weight</code> : same as the above, except for <code>weight</code> in place of <code>origin</code>.</li>
</ul>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">ml_model_1</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"mpg ~ displacement + weight + year : origin"</span>,
                     data=auto<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">ml_model_2</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"mpg ~ displacement + weight + year * origin"</span>,
                     data=auto<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">ml_model_3</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"mpg ~ displacement + weight * year + origin"</span>,
                     data=auto<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
</pre>
</div>
</div>

<ul class="org-ul">
<li><a id="org3e2b1a8"></a>Summary of first interaction model<br />
<div class="outline-text-5" id="text-org3e2b1a8">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model_1.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.714
Model:                            OLS   Adj. R-squared:                  0.712
Method:                 Least Squares   F-statistic:                     322.7
Date:                Mon, 25 May 2020   Prob (F-statistic):          4.85e-105
Time:                        18:13:04   Log-Likelihood:                -1115.9
No. Observations:                 392   AIC:                             2240.
Df Residuals:                     388   BIC:                             2256.
Df Model:                           3
Covariance Type:            nonrobust
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
Intercept       39.8822      1.427     27.939      0.000      37.076      42.689
displacement    -0.0100      0.006     -1.728      0.085      -0.021       0.001
weight          -0.0056      0.001     -8.137      0.000      -0.007      -0.004
year:origin      0.0193      0.004      4.502      0.000       0.011       0.028
==============================================================================
Omnibus:                       41.720   Durbin-Watson:                   0.883
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               68.034
Skew:                           0.674   Prob(JB):                     1.69e-15
Kurtosis:                       4.532   Cond. No.                     2.09e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.09e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<p>
The large F-statistic invalidates the null hypothesis. The individual p-values
show that <code>displacement</code> is not really significant, but the product of <code>year</code>
and <code>origin</code> is. Additionally the \(R^2\) value tells us that this model explains
around 71% of the <code>mpg</code> values.
</p>
</div>
</li>

<li><a id="orgf90da35"></a>Summary of second interaction model<br />
<div class="outline-text-5" id="text-orgf90da35">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model_2.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.823
Model:                            OLS   Adj. R-squared:                  0.821
Method:                 Least Squares   F-statistic:                     359.5
Date:                Mon, 25 May 2020   Prob (F-statistic):          8.65e-143
Time:                        18:17:24   Log-Likelihood:                -1021.6
No. Observations:                 392   AIC:                             2055.
Df Residuals:                     386   BIC:                             2079.
Df Model:                           5
Covariance Type:            nonrobust
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
Intercept        7.9270      8.873      0.893      0.372      -9.519      25.373
displacement     0.0016      0.005      0.319      0.750      -0.008       0.011
weight          -0.0064      0.001    -11.571      0.000      -0.007      -0.005
year             0.4313      0.113      3.818      0.000       0.209       0.653
origin         -14.4936      4.707     -3.079      0.002     -23.749      -5.239
year:origin      0.2023      0.060      3.345      0.001       0.083       0.321
==============================================================================
Omnibus:                       38.636   Durbin-Watson:                   1.322
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               71.804
Skew:                           0.584   Prob(JB):                     2.56e-16
Kurtosis:                       4.741   Cond. No.                     1.84e+05
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.84e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<p>
The \(R^2\) value has increased; it is now almost the same as the \(R^2\) value
for the model with all the quantitative predictors but no interaction. This
model explains around 82% of the <code>mpg</code> values. Additionally we see that <code>year</code>
is more significant than <code>origin</code> or the product of <code>year</code> and <code>origin</code>. Also,
in addition to <code>displacement</code>, the intercept term appears to be insignificant
too.
</p>
</div>
</li>

<li><a id="org86bdfe9"></a>Summary of third interaction model<br />
<div class="outline-text-5" id="text-org86bdfe9">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model_3.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.840
Model:                            OLS   Adj. R-squared:                  0.838
Method:                 Least Squares   F-statistic:                     404.4
Date:                Mon, 25 May 2020   Prob (F-statistic):          5.53e-151
Time:                        18:22:57   Log-Likelihood:                -1002.4
No. Observations:                 392   AIC:                             2017.
Df Residuals:                     386   BIC:                             2041.
Df Model:                           5
Covariance Type:            nonrobust
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
Intercept     -107.6004     12.904     -8.339      0.000    -132.971     -82.229
displacement    -0.0004      0.005     -0.088      0.930      -0.009       0.009
weight           0.0260      0.005      5.722      0.000       0.017       0.035
year             1.9624      0.172     11.436      0.000       1.625       2.300
weight:year     -0.0004   5.97e-05     -7.214      0.000      -0.001      -0.000
origin           0.9116      0.255      3.579      0.000       0.411       1.412
==============================================================================
Omnibus:                       43.792   Durbin-Watson:                   1.372
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               89.759
Skew:                           0.619   Prob(JB):                     3.23e-20
Kurtosis:                       4.991   Cond. No.                     1.90e+07
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.9e+07. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<p>
The \(R^2\) increased a bit, and it appears other than <code>displacement</code> all the
other predictors are significant.
</p>
</div>
</li>

<li><a id="org86c8d3c"></a>Interaction model with two interactions<br />
<div class="outline-text-5" id="text-org86c8d3c">
<p>
We will try an additional model with two interactions: <code>displacement * weight</code>
and <code>weight * year</code>.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">ml_model_4</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"mpg ~ displacement * weight + weight * year + origin"</span>,
                     data=auto<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model_4.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.859
Model:                            OLS   Adj. R-squared:                  0.856
Method:                 Least Squares   F-statistic:                     389.6
Date:                Mon, 25 May 2020   Prob (F-statistic):          4.10e-160
Time:                        18:33:39   Log-Likelihood:                -977.80
No. Observations:                 392   AIC:                             1970.
Df Residuals:                     385   BIC:                             1997.
Df Model:                           6
Covariance Type:            nonrobust
=======================================================================================
                          coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------
Intercept             -61.3985     13.741     -4.468      0.000     -88.416     -34.381
displacement           -0.0604      0.009     -6.424      0.000      -0.079      -0.042
weight                  0.0090      0.005      1.848      0.065      -0.001       0.019
displacement:weight  1.708e-05   2.38e-06      7.169      0.000    1.24e-05    2.18e-05
year                    1.4982      0.174      8.616      0.000       1.156       1.840
weight:year            -0.0002   6.16e-05     -4.037      0.000      -0.000      -0.000
origin                  0.3388      0.253      1.342      0.181      -0.158       0.835
==============================================================================
Omnibus:                       62.892   Durbin-Watson:                   1.412
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              174.597
Skew:                           0.754   Prob(JB):                     1.22e-38
Kurtosis:                       5.901   Cond. No.                     8.00e+07
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large,  8e+07. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<p>
This is interesting. We see that the \(R^2\) value has increased further, but now
<code>displacement</code> has become significant whereas <code>weight</code> and <code>origin</code> have become
relatively insignificant. The interaction terms are still significant. My
understanding of this model is that while <code>weight</code> does not directly affect
<code>mpg</code>, it increases <code>displacement</code>, and that affects the <code>mpg</code>.
</p>
</div>
</li>
</ul>
</div>

<div id="outline-container-org1f3ccc0" class="outline-4">
<h4 id="org1f3ccc0">Models with variable transformations</h4>
<div class="outline-text-4" id="text-org1f3ccc0">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">ml_model_trans</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"mpg ~ np.log(weight) + np.power(weight, 2) + year"</span>,
                         data=auto<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model_trans.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.851
Model:                            OLS   Adj. R-squared:                  0.850
Method:                 Least Squares   F-statistic:                     749.0
Date:                Mon, 25 May 2020   Prob (F-statistic):          4.19e-162
Time:                        18:49:50   Log-Likelihood:                -1001.5
No. Observations:                 397   AIC:                             2011.
Df Residuals:                     393   BIC:                             2027.
Df Model:                           3
Covariance Type:            nonrobust
=======================================================================================
                          coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------
Intercept             213.1803     16.063     13.271      0.000     181.599     244.761
np.log(weight)        -32.5971      2.150    -15.159      0.000     -36.825     -28.369
np.power(weight, 2)  6.506e-07   1.12e-07      5.804      0.000     4.3e-07    8.71e-07
year                    0.8355      0.044     19.023      0.000       0.749       0.922
==============================================================================
Omnibus:                       69.088   Durbin-Watson:                   1.361
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              168.823
Skew:                           0.864   Prob(JB):                     2.19e-37
Kurtosis:                       5.687   Cond. No.                     1.17e+09
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.17e+09. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<p>
The F-statistic and the p-values indicate that these transformations are
statistically significant.
</p>
</div>
</div>
</div>

<div id="outline-container-org0878758" class="outline-3">
<h3 id="org0878758">Question 10</h3>
<div class="outline-text-3" id="text-org0878758">
</div>
<div id="outline-container-org586eb8d" class="outline-4">
<h4 id="org586eb8d">Multiple regression with <code>Carseats</code> data set</h4>
<div class="outline-text-4" id="text-org586eb8d">
<p>
So far I had been loading the data sets from local <code>.csv</code> files, but I recently
found out that <code>statsmodels</code> makes them automatically available using the
<a href="http://vincentarelbundock.github.io/Rdatasets/datasets.html">Rdatasets project</a>. So going forward I will be using that whenever possible.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.api <span style="color: #859900; font-weight: bold;">as</span> sm

<span style="color: #268bd2;">carseats</span> = sm.datasets.get_rdataset<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Carseats"</span>, package=<span style="color: #2aa198;">"ISLR"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>carseats.<span style="color: #d33682; font-style: italic;">__doc__</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
+----------+-----------------+
| Carseats | R Documentation |
+----------+-----------------+

Sales of Child Car Seats
------------------------

Description
~~~~~~~~~~~

A simulated data set containing sales of child car seats at 400
different stores.

Usage
~~~~~

::

   Carseats

Format
~~~~~~

A data frame with 400 observations on the following 11 variables.

``Sales``
   Unit sales (in thousands) at each location

``CompPrice``
   Price charged by competitor at each location

``Income``
   Community income level (in thousands of dollars)

``Advertising``
   Local advertising budget for company at each location (in thousands
   of dollars)

``Population``
   Population size in region (in thousands)

``Price``
   Price company charges for car seats at each site

``ShelveLoc``
   A factor with levels ``Bad``, ``Good`` and ``Medium`` indicating the
   quality of the shelving location for the car seats at each site

``Age``
   Average age of the local population

``Education``
   Education level at each location

``Urban``
   A factor with levels ``No`` and ``Yes`` to indicate whether the store
   is in an urban or rural location

``US``
   A factor with levels ``No`` and ``Yes`` to indicate whether the store
   is in the US or not

Source
~~~~~~

Simulated data

References
~~~~~~~~~~

James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) *An
Introduction to Statistical Learning with applications in R*,
`www.StatLearning.com &lt;www.StatLearning.com&gt;`__, Springer-Verlag, New
York

Examples
~~~~~~~~

::

   summary(Carseats)
   lm.fit=lm(Sales~Advertising+Price,data=Carseats)
</pre>

<p>
Multiple linear regression to predict <code>Sales</code> using <code>Price</code>, <code>Urban</code>, and <code>US</code>.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.formula.api <span style="color: #859900; font-weight: bold;">as</span> smf

<span style="color: #268bd2;">model</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"Sales ~ Price + Urban + US"</span>,
                data=carseats.data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>

<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                  Sales   R-squared:                       0.239
Model:                            OLS   Adj. R-squared:                  0.234
Method:                 Least Squares   F-statistic:                     41.52
Date:                Thu, 28 May 2020   Prob (F-statistic):           2.39e-23
Time:                        18:21:58   Log-Likelihood:                -927.66
No. Observations:                 400   AIC:                             1863.
Df Residuals:                     396   BIC:                             1879.
Df Model:                           3
Covariance Type:            nonrobust
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
Intercept       13.0435      0.651     20.036      0.000      11.764      14.323
Urban[T.Yes]    -0.0219      0.272     -0.081      0.936      -0.556       0.512
US[T.Yes]        1.2006      0.259      4.635      0.000       0.691       1.710
Price           -0.0545      0.005    -10.389      0.000      -0.065      -0.044
==============================================================================
Omnibus:                        0.676   Durbin-Watson:                   1.912
Prob(Omnibus):                  0.713   Jarque-Bera (JB):                0.758
Skew:                           0.093   Prob(JB):                        0.684
Kurtosis:                       2.897   Cond. No.                         628.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The F-statistic is larger than 1, though <b>much</b> smaller compared to the
F-statistics in the last problem. I think this means that the alternative
hypothesis is viable, but not completely sure about that.
</p>

<p>
From the individual p-values we can conclude that <code>Urban</code> is not a statistically
significant predictor for <code>Sales</code>.
</p>
</div>
</div>

<div id="outline-container-org5ec70bf" class="outline-4">
<h4 id="org5ec70bf">Interpretation of coefficient of predictors</h4>
<div class="outline-text-4" id="text-org5ec70bf">
<p>
Since <code>Urban</code> is not a statistically significant predictor we do not need to
worry about its coefficient. The coefficient for <code>US</code> indicates that if the
store is in the US then it then the sales will increase by about 1200 units. On
the other hand the coefficient for <code>Price</code> says that an increase in price will
result in a decrease in sales.
</p>
</div>
</div>

<div id="outline-container-orgd7b053c" class="outline-4">
<h4 id="orgd7b053c">Linear model equation</h4>
<div class="outline-text-4" id="text-orgd7b053c">
<p>
The equation for this model is
</p>
\begin{align}
  Y = 13.04 - 0.02 X_1 + 1.20 X_2 - 0.05 X_3,
\end{align}
<p>
where \(Y\), \(X_1\), \(X_2\), and \(X_3\) stand for <code>Sales</code>, <code>Urban</code>, <code>US</code>, and
<code>Price</code>, respectively. \(X_1 = 1\) if the store is an urban location, and \(0\)
otherwise. Similarly \(X_2 = 1\) if the store is in the US, and \(0\) if it is
not.
</p>
</div>
</div>

<div id="outline-container-org392ea7c" class="outline-4">
<h4 id="org392ea7c">Null hypothesis rejection</h4>
<div class="outline-text-4" id="text-org392ea7c">
<p>
We can reject the null hypothesis for <code>US</code>, and <code>Price</code>, since the associated
p-values are effectively 0.
</p>
</div>
</div>

<div id="outline-container-orgc75a2b3" class="outline-4">
<h4 id="orgc75a2b3">Smaller multiple linear model for <code>Carseats</code> sales</h4>
<div class="outline-text-4" id="text-orgc75a2b3">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">small_model</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"Sales ~ US + Price"</span>, data=carseats.data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>small_model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                  Sales   R-squared:                       0.239
Model:                            OLS   Adj. R-squared:                  0.235
Method:                 Least Squares   F-statistic:                     62.43
Date:                Thu, 28 May 2020   Prob (F-statistic):           2.66e-24
Time:                        18:45:15   Log-Likelihood:                -927.66
No. Observations:                 400   AIC:                             1861.
Df Residuals:                     397   BIC:                             1873.
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     13.0308      0.631     20.652      0.000      11.790      14.271
US[T.Yes]      1.1996      0.258      4.641      0.000       0.692       1.708
Price         -0.0545      0.005    -10.416      0.000      -0.065      -0.044
==============================================================================
Omnibus:                        0.666   Durbin-Watson:                   1.912
Prob(Omnibus):                  0.717   Jarque-Bera (JB):                0.749
Skew:                           0.092   Prob(JB):                        0.688
Kurtosis:                       2.895   Cond. No.                         607.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
Based on the \(R^2\) values both the models fit the data similarly.
</p>
</div>
</div>

<div id="outline-container-orgc2abcaa" class="outline-4">
<h4 id="orgc2abcaa">Confidence intervals of fitted parameters</h4>
<div class="outline-text-4" id="text-orgc2abcaa">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>small_model.conf_int<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                  0          1
Intercept  11.79032  14.271265
US[T.Yes]   0.69152   1.707766
Price      -0.06476  -0.044195

</pre>

<p>
The confidence intervals are also printed in the summary, but this is probably
more convenient.
</p>
</div>
</div>

<div id="outline-container-org16ec8fc" class="outline-4">
<h4 id="org16ec8fc">Outliers and leverages</h4>
<div class="outline-text-4" id="text-org16ec8fc">
<p>
To see if there are any leverage points we need to first calculate the average
leverage, \((p + 1) / n\), for the data.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">npredictors</span> = 2
<span style="color: #268bd2;">nobservations</span> = <span style="color: #d33682; font-style: italic;">len</span><span style="color: #268bd2;">(</span>carseats.data<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">avg_leverage</span> = <span style="color: #268bd2;">(</span>npredictors + 1<span style="color: #268bd2;">)</span> / nobservations

<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"Average leverage: {avg_leverage}"</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
Average leverage: 0.0075

</pre>

<p>
The <b>Residuals vs Leverage</b> plot is the easiest way to check for outliers and
high leverage observations.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #859900; font-weight: bold;">as</span> plt
<span style="color: #859900; font-weight: bold;">import</span> seaborn <span style="color: #859900; font-weight: bold;">as</span> sns

sns.set_style<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"ticks"</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">rlplot</span> = sm.graphics.influence_plot<span style="color: #268bd2;">(</span>small_model, criterion=<span style="color: #2aa198;">"Cooks"</span>, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Leverage"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
ax.set_title<span style="color: #268bd2;">(</span><span style="color: #2aa198;">" "</span><span style="color: #268bd2;">)</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.10.h_res_vs_lev.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.10.h_res_vs_lev.png" alt="3.10.h_res_vs_lev.png" />
</p>
</div>

<p>
All the residuals are between -3 and 3, so there are no outliers. However there
are a lot of points whose leverage greatly exceeds the average leverage. Thus
there are high leverage observations.
</p>
</div>
</div>
</div>

<div id="outline-container-org6b1e95a" class="outline-3">
<h3 id="org6b1e95a">Question 11</h3>
<div class="outline-text-3" id="text-org6b1e95a">
</div>
<div id="outline-container-org4b36e9c" class="outline-4">
<h4 id="org4b36e9c">Simple linear regression with synthetic data</h4>
<div class="outline-text-4" id="text-org4b36e9c">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> pandas <span style="color: #859900; font-weight: bold;">as</span> pd
<span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #859900; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #859900; font-weight: bold;">as</span> plt
<span style="color: #859900; font-weight: bold;">import</span> seaborn <span style="color: #859900; font-weight: bold;">as</span> sns

sns.set_style<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"ticks"</span><span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">rng</span> = np.random.default_rng<span style="color: #268bd2;">(</span>seed=42<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x</span> = rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">y</span> = 2 * x + rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">data</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"X"</span> : x, <span style="color: #2aa198;">"Y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">sp</span> = sns.scatterplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"X"</span>, y=<span style="color: #2aa198;">"Y"</span>, data=data, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.11.a_data.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.11.a_data.png" alt="3.11.a_data.png" />
</p>
</div>

<p>
Now we do a simple linear regression with this synthetic data. This model will
not have an intercept: \(Y = Î²X\).
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.formula.api <span style="color: #859900; font-weight: bold;">as</span> smf

<span style="color: #268bd2;">model</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Y ~ X - 1"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                                 OLS Regression Results
=======================================================================================
Dep. Variable:                      Y   R-squared (uncentered):                   0.741
Model:                            OLS   Adj. R-squared (uncentered):              0.738
Method:                 Least Squares   F-statistic:                              283.3
Date:                Fri, 29 May 2020   Prob (F-statistic):                    8.30e-31
Time:                        07:28:26   Log-Likelihood:                         -138.87
No. Observations:                 100   AIC:                                      279.7
Df Residuals:                      99   BIC:                                      282.4
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
X              2.1196      0.126     16.833      0.000       1.870       2.369
==============================================================================
Omnibus:                        2.995   Durbin-Watson:                   1.681
Prob(Omnibus):                  0.224   Jarque-Bera (JB):                2.970
Skew:                           0.408   Prob(JB):                        0.227
Kurtosis:                       2.787   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The coefficient estimate is \(\hat{Î²} = 2.1196\) with a standard error of
\(0.126\). The t-statistic is 16.833 and the associated p-value is 0. This means
we can reject the null hypothesis.
</p>
</div>
</div>

<div id="outline-container-org13982fb" class="outline-4">
<h4 id="org13982fb">Inverse simple linear relation with synthetic data</h4>
<div class="outline-text-4" id="text-org13982fb">
<p>
We are going to use the same data, but now with <code>X</code> as the response and <code>Y</code> as
the predictor.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">model2</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"X ~ Y - 1"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model2.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                                 OLS Regression Results
=======================================================================================
Dep. Variable:                      X   R-squared (uncentered):                   0.741
Model:                            OLS   Adj. R-squared (uncentered):              0.738
Method:                 Least Squares   F-statistic:                              283.3
Date:                Fri, 29 May 2020   Prob (F-statistic):                    8.30e-31
Time:                        07:36:14   Log-Likelihood:                         -48.770
No. Observations:                 100   AIC:                                      99.54
Df Residuals:                      99   BIC:                                      102.1
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Y              0.3496      0.021     16.833      0.000       0.308       0.391
==============================================================================
Omnibus:                        1.369   Durbin-Watson:                   1.557
Prob(Omnibus):                  0.504   Jarque-Bera (JB):                1.145
Skew:                          -0.020   Prob(JB):                        0.564
Kurtosis:                       2.477   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The coefficient estimate is \(\hat{Î²} = 0.3496\) with a standard error of
\(0.021\). The t-statistic is \(16.833\) and the associated p-value is 0. This means
we can reject the null hypothesis.
</p>
</div>
</div>

<div id="outline-container-org0005b62" class="outline-4">
<h4 id="org0005b62">Relation between the two regressions</h4>
<div class="outline-text-4" id="text-org0005b62">
<p>
Given the underlying true model we should have expected that the coefficients of
the two models would be multiplicative inverses of each other. But they are not.
The reason being that the two models are minimizing different residual sum of
squares. For the two models the residual sum of squares are
</p>
\begin{align}
  \mathrm{RSS}^{(1)} &= â_{i=1}^n (y_i - \hat{Î²}^(1) x_i)^2, \\
  \mathrm{RSS}^{(2)} &= â_{i=1}^n (x_i - \hat{Î²}^(2) y_i)^2,
\end{align}
<p>
respectively. \(\mathrm{RSS}^(1)\) is minimized when \(\hat{Î²}^(1) = ây_i x_i / â
x_i^2\), and \(\mathrm{RSS}^(2)\) is minimized when \(\hat{Î²}^(2) = âx_i y_i / â
y_i^2\). If \(\hat{Î²}^(1) = 1 / \hat{Î²}^(2)\) then we have
</p>
\begin{align}
  (â_{i=1}^n x_i y_i)^2 = â_{i=1}^n x_i^2 â_{i=1}^n y_i^2.
\end{align}
<p>
Since here \(X\) and \(Y\) are random variables with zero mean we can interpret
the above equation as
</p>
\begin{align}
  \mathrm{Cov}(X, Y) = \mathrm{Var}(X) \mathrm{Var}(Y).
\end{align}
<p>
This is true only if the true relation is \(y_i = Î² x_i + Î³\) for some nonzero
constants \(Î²\) and \(Î³\) (See <a href="http://bio5495.wustl.edu/Probability/Readings/DeGroot4thEdition.pdf">DeGroot and Schervish, Theorem 4.6.3</a>, or
<a href="https://proofwiki.org/wiki/Square_of_Covariance_is_Less_Than_or_Equal_to_Product_of_Variances">ProofWiki</a> for a proof of this statement.). But the true relation in this case
was \(y_i = Î² x_i + Ïµ\), where \(Ïµ\) is a Gaussian random variable with zero mean.
Thus the above statement is not true, and hence \(\hat{Î²}^(1) â  1 / \hat{Î²}^(2)\).
For a more detailed discussion on this check <a href="https://stats.stackexchange.com/questions/20553/effect-of-switching-response-and-explanatory-variable-in-simple-linear-regressio/20560#20560">Stats StackExchange</a>.
</p>
</div>
</div>

<div id="outline-container-orgfb9ddec" class="outline-4">
<h4 id="orgfb9ddec">t-statistic for first model</h4>
<div class="outline-text-4" id="text-orgfb9ddec">
<p>
The t-statistic for a simple linear fit without intercept is \(\hat{Î²} /
\mathrm{SE}(\hat{Î²})\) where \(\hat{Î²} = â_i x_i y_i / â_i x_i^2\), and the
standard error is
</p>
\begin{align}
  \mathrm{SE}(\hat{Î²}
  = \frac{\sqrt{â_i (y_i - x_i \hat{Î²})^2}}{(n-1) â_i x_i^2}.
\end{align}
<p>
Substituting the expression for \(\hat{Î²}\) in to the expressions for the
standard error and the t-statistic gives us the expected expression for the
t-statistic. The trick is to realize that the summation indices are dummy
variables, i.e. \(â_{i=1}^n x_i^2 = â_{j=1}^n x_j^2\).
Numerically we can conform this as follows:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">n</span> = <span style="color: #d33682; font-style: italic;">len</span><span style="color: #268bd2;">(</span>data<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x</span>, <span style="color: #268bd2;">y</span> = data<span style="color: #268bd2;">[</span><span style="color: #2aa198;">"X"</span><span style="color: #268bd2;">]</span>, data<span style="color: #268bd2;">[</span><span style="color: #2aa198;">"Y"</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">t</span> = <span style="color: #268bd2;">(</span>np.sqrt<span style="color: #d33682;">(</span>n - 1<span style="color: #d33682;">)</span> * np.<span style="color: #d33682; font-style: italic;">sum</span><span style="color: #d33682;">(</span>x * y<span style="color: #d33682;">)</span>
     / np.sqrt<span style="color: #d33682;">(</span>np.<span style="color: #d33682; font-style: italic;">sum</span><span style="color: #859900;">(</span>x ** 2<span style="color: #859900;">)</span> * np.<span style="color: #d33682; font-style: italic;">sum</span><span style="color: #859900;">(</span>y ** 2<span style="color: #859900;">)</span> - np.<span style="color: #d33682; font-style: italic;">sum</span><span style="color: #859900;">(</span>x * y<span style="color: #859900;">)</span> ** 2<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"t-statistic: {t:.3f}"</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
t-statistic: 16.833

</pre>
</div>
</div>

<div id="outline-container-orge972263" class="outline-4">
<h4 id="orge972263">t-statistic for second model</h4>
<div class="outline-text-4" id="text-orge972263">
<p>
The expression for the t-statistic is symmetric in \(X\) and \(Y\), so
irrespective of whether we are regressing \(Y\) onto \(X\) or \(X\) onto \(Y\),
we will have the same t-statistic.
</p>
</div>
</div>

<div id="outline-container-orgf76947f" class="outline-4">
<h4 id="orgf76947f">t-statistic for models with intercept</h4>
<div class="outline-text-4" id="text-orgf76947f">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">model3</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"Y ~ X"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">model4</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"X ~ Y"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model3.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.740
Model:                            OLS   Adj. R-squared:                  0.738
Method:                 Least Squares   F-statistic:                     279.2
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.94e-30
Time:                        00:26:31   Log-Likelihood:                -138.87
No. Observations:                 100   AIC:                             281.7
Df Residuals:                      98   BIC:                             287.0
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0046      0.098     -0.047      0.962      -0.200       0.190
X              2.1192      0.127     16.709      0.000       1.867       2.371
==============================================================================
Omnibus:                        2.996   Durbin-Watson:                   1.682
Prob(Omnibus):                  0.224   Jarque-Bera (JB):                2.971
Skew:                           0.409   Prob(JB):                        0.226
Kurtosis:                       2.787   Cond. No.                         1.30
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model4.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      X   R-squared:                       0.740
Model:                            OLS   Adj. R-squared:                  0.738
Method:                 Least Squares   F-statistic:                     279.2
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.94e-30
Time:                        00:26:47   Log-Likelihood:                -48.728
No. Observations:                 100   AIC:                             101.5
Df Residuals:                      98   BIC:                             106.7
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0114      0.040     -0.287      0.775      -0.091       0.068
Y              0.3493      0.021     16.709      0.000       0.308       0.391
==============================================================================
Omnibus:                        1.373   Durbin-Watson:                   1.559
Prob(Omnibus):                  0.503   Jarque-Bera (JB):                1.146
Skew:                          -0.018   Prob(JB):                        0.564
Kurtosis:                       2.477   Cond. No.                         1.91
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
We can see that the t-coefficient for the predictors is same for both models.
</p>
</div>
</div>
</div>

<div id="outline-container-org56e7d6c" class="outline-3">
<h3 id="org56e7d6c">Question 12</h3>
<div class="outline-text-3" id="text-org56e7d6c">
</div>
<div id="outline-container-org51cb211" class="outline-4">
<h4 id="org51cb211">Equal regression coefficients</h4>
<div class="outline-text-4" id="text-org51cb211">
<p>
As this is a regression without intercept we can use the expressions derived in
the previous question. The coefficients will be same when \(â_i x_i^2 = â_i
y_i^2\). This is particularly true when \(X = Y\).
</p>
</div>
</div>

<div id="outline-container-orgcd46063" class="outline-4">
<h4 id="orgcd46063">Different coefficient estimates - numerical</h4>
<div class="outline-text-4" id="text-orgcd46063">
<p>
Essentially reusing question 11.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">rng</span> = np.random.default_rng<span style="color: #268bd2;">(</span>seed=42<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x</span> = rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">y</span> = 2 * x + rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">data</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"X"</span> : x, <span style="color: #2aa198;">"Y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">sp</span> = sns.scatterplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"X"</span>, y=<span style="color: #2aa198;">"Y"</span>, data=data, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.12.b_data.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.12.b_data.png" alt="3.12.b_data.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">model1</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Y ~ X - 1"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">model2</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"X ~ Y - 1"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model1.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                                 OLS Regression Results
=======================================================================================
Dep. Variable:                      Y   R-squared (uncentered):                   0.741
Model:                            OLS   Adj. R-squared (uncentered):              0.738
Method:                 Least Squares   F-statistic:                              283.3
Date:                Sat, 30 May 2020   Prob (F-statistic):                    8.30e-31
Time:                        00:47:09   Log-Likelihood:                         -138.87
No. Observations:                 100   AIC:                                      279.7
Df Residuals:                      99   BIC:                                      282.4
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
X              2.1196      0.126     16.833      0.000       1.870       2.369
==============================================================================
Omnibus:                        2.995   Durbin-Watson:                   1.681
Prob(Omnibus):                  0.224   Jarque-Bera (JB):                2.970
Skew:                           0.408   Prob(JB):                        0.227
Kurtosis:                       2.787   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model2.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                                 OLS Regression Results
=======================================================================================
Dep. Variable:                      X   R-squared (uncentered):                   0.741
Model:                            OLS   Adj. R-squared (uncentered):              0.738
Method:                 Least Squares   F-statistic:                              283.3
Date:                Sat, 30 May 2020   Prob (F-statistic):                    8.30e-31
Time:                        00:47:17   Log-Likelihood:                         -48.770
No. Observations:                 100   AIC:                                      99.54
Df Residuals:                      99   BIC:                                      102.1
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Y              0.3496      0.021     16.833      0.000       0.308       0.391
==============================================================================
Omnibus:                        1.369   Durbin-Watson:                   1.557
Prob(Omnibus):                  0.504   Jarque-Bera (JB):                1.145
Skew:                          -0.020   Prob(JB):                        0.564
Kurtosis:                       2.477   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>
</div>
</div>

<div id="outline-container-org14b71fd" class="outline-4">
<h4 id="org14b71fd">Same coefficient estimates - numerical</h4>
<div class="outline-text-4" id="text-org14b71fd">
<p>
We need \(â_i x_i^2 = â_i y_i^2\). Setting \(X = Y\) works, but \(Y =
\mathrm{Permutation}(X)\) would work too, and is more general.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">rng</span> = np.random.default_rng<span style="color: #268bd2;">(</span>seed=42<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x</span> = rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">y</span> = np.random.permutation<span style="color: #268bd2;">(</span>x<span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">data</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"X"</span> : x, <span style="color: #2aa198;">"Y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">sp</span> = sns.scatterplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"X"</span>, y=<span style="color: #2aa198;">"Y"</span>, data=data, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.12.b_data.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="./.ob-jupyter/622965e34dd64910e6e87714a6ee768cf7006833.png" alt="622965e34dd64910e6e87714a6ee768cf7006833.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">model3</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Y ~ X - 1"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">model4</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"X ~ Y - 1"</span>, data=data<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model3.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                                 OLS Regression Results
=======================================================================================
Dep. Variable:                      Y   R-squared (uncentered):                   0.002
Model:                            OLS   Adj. R-squared (uncentered):             -0.008
Method:                 Least Squares   F-statistic:                             0.2018
Date:                Sat, 30 May 2020   Prob (F-statistic):                       0.654
Time:                        00:52:14   Log-Likelihood:                         -116.23
No. Observations:                 100   AIC:                                      234.5
Df Residuals:                      99   BIC:                                      237.1
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
X              0.0451      0.100      0.449      0.654      -0.154       0.244
==============================================================================
Omnibus:                        0.651   Durbin-Watson:                   1.772
Prob(Omnibus):                  0.722   Jarque-Bera (JB):                0.787
Skew:                          -0.142   Prob(JB):                        0.675
Kurtosis:                       2.671   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model4.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                                 OLS Regression Results
=======================================================================================
Dep. Variable:                      X   R-squared (uncentered):                   0.002
Model:                            OLS   Adj. R-squared (uncentered):             -0.008
Method:                 Least Squares   F-statistic:                             0.2018
Date:                Sat, 30 May 2020   Prob (F-statistic):                       0.654
Time:                        00:52:20   Log-Likelihood:                         -116.23
No. Observations:                 100   AIC:                                      234.5
Df Residuals:                      99   BIC:                                      237.1
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Y              0.0451      0.100      0.449      0.654      -0.154       0.244
==============================================================================
Omnibus:                        0.296   Durbin-Watson:                   1.833
Prob(Omnibus):                  0.862   Jarque-Bera (JB):                0.446
Skew:                          -0.105   Prob(JB):                        0.800
Kurtosis:                       2.749   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>
</div>
</div>
</div>

<div id="outline-container-org1d7b4c8" class="outline-3">
<h3 id="org1d7b4c8">Question 13</h3>
<div class="outline-text-3" id="text-org1d7b4c8">
</div>
<div id="outline-container-org4cb4e2a" class="outline-4">
<h4 id="org4cb4e2a">Feature vector</h4>
<div class="outline-text-4" id="text-org4cb4e2a">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">rng</span> = np.random.default_rng<span style="color: #268bd2;">(</span>seed=42<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x</span> = rng.normal<span style="color: #268bd2;">(</span>loc=0, scale=1, size=100<span style="color: #268bd2;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org4e47f56" class="outline-4">
<h4 id="org4e47f56">Error vector</h4>
<div class="outline-text-4" id="text-org4e47f56">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">eps</span> = rng.normal<span style="color: #268bd2;">(</span>loc=0, scale=0.25, size=100<span style="color: #268bd2;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org3b09a3a" class="outline-4">
<h4 id="org3b09a3a">Response vector</h4>
<div class="outline-text-4" id="text-org3b09a3a">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">y</span> = -1 + 0.5 * x + eps
</pre>
</div>

<p>
The length of <code>y</code> is 100, and \(Î²_0 = -1\), and \(Î²_1 = 0.5\).
</p>
</div>
</div>

<div id="outline-container-org50682f4" class="outline-4">
<h4 id="org50682f4">Scatter plot</h4>
<div class="outline-text-4" id="text-org50682f4">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">df</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"x"</span> : x, <span style="color: #2aa198;">"y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">sp</span> = sns.jointplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"x"</span>, y=<span style="color: #2aa198;">"y"</span>, data=df<span style="color: #268bd2;">)</span> <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">jointplot also gives the distributions of x and y in addition to the scatter plot</span>
sns.despine<span style="color: #268bd2;">()</span>
sp.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.13.d_scatter.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.13.d_scatter.png" alt="3.13.d_scatter.png" />
</p>
</div>

<p>
We can see a clear linear trend between <code>x</code> and <code>y</code>.
</p>
</div>
</div>

<div id="outline-container-orgd2b17d0" class="outline-4">
<h4 id="orgd2b17d0">Least squares fit</h4>
<div class="outline-text-4" id="text-orgd2b17d0">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">model</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x"</span>, data=df<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.740
Model:                            OLS   Adj. R-squared:                  0.738
Method:                 Least Squares   F-statistic:                     279.2
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.94e-30
Time:                        03:07:58   Log-Likelihood:               -0.24351
No. Observations:                 100   AIC:                             4.487
Df Residuals:                      98   BIC:                             9.697
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -1.0012      0.025    -40.774      0.000      -1.050      -0.952
x              0.5298      0.032     16.709      0.000       0.467       0.593
==============================================================================
Omnibus:                        2.996   Durbin-Watson:                   1.682
Prob(Omnibus):                  0.224   Jarque-Bera (JB):                2.971
Skew:                           0.409   Prob(JB):                        0.226
Kurtosis:                       2.787   Cond. No.                         1.30
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>


<p>
The estimates for \(Î²_0\) and \(Î²_1\) are almost equal to the true values. The
true values fall within the 95% confidence interval of the estimated values.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">ypred</span> = model.predict<span style="color: #268bd2;">(</span>df<span style="color: #d33682;">[</span><span style="color: #2aa198;">"x"</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
ax.plot<span style="color: #268bd2;">(</span>x, y, <span style="color: #2aa198;">'o'</span>, label=<span style="color: #2aa198;">"Data"</span><span style="color: #268bd2;">)</span>
ax.plot<span style="color: #268bd2;">(</span>x, ypred, <span style="color: #2aa198;">'r'</span>, label=<span style="color: #2aa198;">"Least Squares Regression"</span><span style="color: #268bd2;">)</span>
ax.legend<span style="color: #268bd2;">(</span>loc=<span style="color: #2aa198;">"best"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.13.f_ols.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.13.f_ols.png" alt="3.13.f_ols.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orga4cbe44" class="outline-4">
<h4 id="orga4cbe44">Polynomial regression</h4>
<div class="outline-text-4" id="text-orga4cbe44">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">poly_model</span> = smf.ols<span style="color: #268bd2;">(</span>formula=<span style="color: #2aa198;">"y ~ x + I(x**2)"</span>, data=df<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>poly_model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.746
Model:                            OLS   Adj. R-squared:                  0.741
Method:                 Least Squares   F-statistic:                     142.6
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.30e-29
Time:                        03:32:56   Log-Likelihood:                0.93852
No. Observations:                 100   AIC:                             4.123
Df Residuals:                      97   BIC:                             11.94
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.9732      0.031    -31.881      0.000      -1.034      -0.913
x              0.5200      0.032     16.177      0.000       0.456       0.584
I(x ** 2)     -0.0474      0.031     -1.523      0.131      -0.109       0.014
==============================================================================
Omnibus:                        2.591   Durbin-Watson:                   1.731
Prob(Omnibus):                  0.274   Jarque-Bera (JB):                2.542
Skew:                           0.380   Prob(JB):                        0.281
Kurtosis:                       2.818   Cond. No.                         2.08
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The \(R^2\) values of both the models are pretty much the same. Additionally the
p-value of the quadratic term is not zero. The quadratic term does not improve
the model fit.
</p>
</div>
</div>

<div id="outline-container-orgb84e7ca" class="outline-4">
<h4 id="orgb84e7ca">Least squares fit with less noise</h4>
<div class="outline-text-4" id="text-orgb84e7ca">
<p>
The new data is as follows. The spread of the noise is now 0.1 instead of 0.25.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">eps</span> = rng.normal<span style="color: #268bd2;">(</span>loc=0, scale=0.1, size=100<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">y</span> = -1 + 0.5 * x + eps

<span style="color: #268bd2;">df2</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"x"</span> : x, <span style="color: #2aa198;">"y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">sp</span> = sns.jointplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"x"</span>, y=<span style="color: #2aa198;">"y"</span>, data=df2<span style="color: #268bd2;">)</span> <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">jointplot also gives the distributions of x and y in addition to the scatter plot</span>
sns.despine<span style="color: #268bd2;">()</span>
sp.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.13.h_scatter.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.13.h_less_noisy_data.png" alt="3.13.h_less_noisy_data.png" />
</p>
</div>

<p>
Now the least squares fit to this data.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">less_noisy_model</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x"</span>, data=df2<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>less_noisy_model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.935
Model:                            OLS   Adj. R-squared:                  0.934
Method:                 Least Squares   F-statistic:                     1403.
Date:                Sat, 30 May 2020   Prob (F-statistic):           7.01e-60
Time:                        03:39:50   Log-Likelihood:                 86.452
No. Observations:                 100   AIC:                            -168.9
Df Residuals:                      98   BIC:                            -163.7
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -1.0063      0.010    -97.523      0.000      -1.027      -0.986
x              0.4991      0.013     37.458      0.000       0.473       0.526
==============================================================================
Omnibus:                        3.211   Durbin-Watson:                   1.893
Prob(Omnibus):                  0.201   Jarque-Bera (JB):                2.554
Skew:                           0.345   Prob(JB):                        0.279
Kurtosis:                       3.371   Cond. No.                         1.30
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The \(R^2\) value for this data set is much larger than the original data set.
The model is able to explain 93% of the less noisy data, whereas it could only
explain around 70% of the original data set.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">ypred</span> = less_noisy_model.predict<span style="color: #268bd2;">(</span>df2<span style="color: #d33682;">[</span><span style="color: #2aa198;">"x"</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
ax.plot<span style="color: #268bd2;">(</span>x, y, <span style="color: #2aa198;">'o'</span>, label=<span style="color: #2aa198;">"Data"</span><span style="color: #268bd2;">)</span>
ax.plot<span style="color: #268bd2;">(</span>x, ypred, <span style="color: #2aa198;">'r'</span>, label=<span style="color: #2aa198;">"Least Squares Regression"</span><span style="color: #268bd2;">)</span>
ax.legend<span style="color: #268bd2;">(</span>loc=<span style="color: #2aa198;">"best"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.13.h_ols.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.13.h_ols.png" alt="3.13.h_ols.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org1fbef9d" class="outline-4">
<h4 id="org1fbef9d">Least squares fit with more noise</h4>
<div class="outline-text-4" id="text-org1fbef9d">
<p>
The new data is as follows. The spread of the noise is now 0.5 instead of 0.25.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">eps</span> = rng.normal<span style="color: #268bd2;">(</span>loc=0, scale=0.5, size=100<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">y</span> = -1 + 0.5 * x + eps

<span style="color: #268bd2;">df3</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"x"</span> : x, <span style="color: #2aa198;">"y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">sp</span> = sns.jointplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"x"</span>, y=<span style="color: #2aa198;">"y"</span>, data=df3<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
sp.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.13.i_scatter.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.13.i_more_noisy_data.png" alt="3.13.i_more_noisy_data.png" />
</p>
</div>

<p>
Now the least squares fit to this data.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">more_noisy_model</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x"</span>, data=df3<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>more_noisy_model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.430
Model:                            OLS   Adj. R-squared:                  0.424
Method:                 Least Squares   F-statistic:                     74.01
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.29e-13
Time:                        03:45:50   Log-Likelihood:                -75.586
No. Observations:                 100   AIC:                             155.2
Df Residuals:                      98   BIC:                             160.4
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -1.0417      0.052    -19.971      0.000      -1.145      -0.938
x              0.5794      0.067      8.603      0.000       0.446       0.713
==============================================================================
Omnibus:                        0.119   Durbin-Watson:                   1.889
Prob(Omnibus):                  0.942   Jarque-Bera (JB):                0.294
Skew:                          -0.029   Prob(JB):                        0.863
Kurtosis:                       2.741   Cond. No.                         1.30
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The \(R^2\) value for this data set is much smaller than it was for the original
data set. The model is able to explain only 43% of the more noisy data, whereas
it could explain around 70% of the original data.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">ypred</span> = more_noisy_model.predict<span style="color: #268bd2;">(</span>df3<span style="color: #d33682;">[</span><span style="color: #2aa198;">"x"</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
ax.plot<span style="color: #268bd2;">(</span>x, y, <span style="color: #2aa198;">'o'</span>, label=<span style="color: #2aa198;">"Data"</span><span style="color: #268bd2;">)</span>
ax.plot<span style="color: #268bd2;">(</span>x, ypred, <span style="color: #2aa198;">'r'</span>, label=<span style="color: #2aa198;">"Least Squares Regression"</span><span style="color: #268bd2;">)</span>
ax.legend<span style="color: #268bd2;">(</span>loc=<span style="color: #2aa198;">"best"</span><span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.13.i_ols.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.13.i_ols.png" alt="3.13.i_ols.png" />
</p>
</div>

<p>
From the graph it appears that there are possible outliers, which is not
surprising given the spread of the error.
</p>
</div>
</div>

<div id="outline-container-org6267844" class="outline-4">
<h4 id="org6267844">Confidence intervals of the three models</h4>
<div class="outline-text-4" id="text-org6267844">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Confidence interval based on original data set:\n"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"{model.conf_int()}\n"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Confidence interval based on less noisy data set:\n"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"{less_noisy_model.conf_int()}\n"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Confidence interval based on more noisy data set:\n"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"{more_noisy_model.conf_int()}\n"</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
Confidence interval based on original data set:

                  0         1
Intercept -1.049887 -0.952433
x          0.466873  0.592714

Confidence interval based on less noisy data set:

                  0         1
Intercept -1.026756 -0.985803
x          0.472653  0.525535

Confidence interval based on more noisy data set:

                  0         1
Intercept -1.145197 -0.938180
x          0.445752  0.713071
</pre>

<p>
The confidence intervals for the less noisy data set are the tightest and
the confidence intervals for the more noisy data set are the loosest.
</p>
</div>
</div>
</div>

<div id="outline-container-org9dd5f43" class="outline-3">
<h3 id="org9dd5f43">Question 14</h3>
<div class="outline-text-3" id="text-org9dd5f43">
</div>
<div id="outline-container-orga8e527f" class="outline-4">
<h4 id="orga8e527f">Multiple linear model with collinearity</h4>
<div class="outline-text-4" id="text-orga8e527f">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">from</span> numpy.random <span style="color: #859900; font-weight: bold;">import</span> MT19937

<span style="color: #268bd2;">rng</span> = np.random.default_rng<span style="color: #268bd2;">(</span>MT19937<span style="color: #d33682;">(</span>seed=5<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x1</span> = rng.uniform<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">x2</span> = 0.5 * x1 + rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span> / 10
<span style="color: #268bd2;">y</span> = 2 + 2 * x1 + 0.3 * x2 + rng.normal<span style="color: #268bd2;">(</span>size=100<span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">df_coll</span> = pd.DataFrame<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"x1"</span> : x1, <span style="color: #2aa198;">"x2"</span> : x2, <span style="color: #2aa198;">"y"</span> : y<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<p>
The form of the linear model is
</p>
\begin{align}
  Y = 2 + 2 X_1 + 0.3 X_2 + Ïµ.
\end{align}
</div>
</div>

<div id="outline-container-org1ffa385" class="outline-4">
<h4 id="org1ffa385">Correlation scatter plot</h4>
<div class="outline-text-4" id="text-org1ffa385">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">corr</span> = df_coll.corr<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>corr<span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
         x1       x2        y
x1  1.00000  0.76818  0.55569
x2  0.76818  1.00000  0.45415
y   0.55569  0.45415  1.00000

</pre>

<p>
The correlation between <code>x1</code> and <code>x2</code> is 0.768.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">sp</span> = sns.scatterplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"x1"</span>, y=<span style="color: #2aa198;">"x2"</span>, data=df_coll, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.14.a_scatter.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.14.a_scatter.png" alt="3.14.a_scatter.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org5b8872f" class="outline-4">
<h4 id="org5b8872f">Least squares fit with <code>x1</code> and <code>x2</code></h4>
<div class="outline-text-4" id="text-org5b8872f">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">coll_model1</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x1 + x2"</span>, data=df_coll<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coll_model1.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.311
Model:                            OLS   Adj. R-squared:                  0.296
Method:                 Least Squares   F-statistic:                     21.85
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.46e-08
Time:                        04:51:30   Log-Likelihood:                -133.37
No. Observations:                 100   AIC:                             272.7
Df Residuals:                      97   BIC:                             280.6
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.8690      0.194      9.651      0.000       1.485       2.253
x1             2.1749      0.568      3.832      0.000       1.048       3.301
x2             0.4454      0.881      0.505      0.614      -1.304       2.194
==============================================================================
Omnibus:                        0.484   Durbin-Watson:                   1.964
Prob(Omnibus):                  0.785   Jarque-Bera (JB):                0.623
Skew:                          -0.140   Prob(JB):                        0.732
Kurtosis:                       2.734   Cond. No.                         12.2
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The estimated values for the coefficients are 1.869, 2.175, and 0.445 which are
close to the true values, albeit with large standard errors, particularly for
\(\hat{Î²}_2\). Based on the p-values we can reject the null hypothesis for
\(Î²_1\), but we cannot reject the null-hypothesis for \(Î²_2\).
</p>
</div>
</div>

<div id="outline-container-org3e3abee" class="outline-4">
<h4 id="org3e3abee">Least squares fit with <code>x1</code> only</h4>
<div class="outline-text-4" id="text-org3e3abee">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">coll_model2</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x1"</span>, data=df_coll<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coll_model2.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.309
Model:                            OLS   Adj. R-squared:                  0.302
Method:                 Least Squares   F-statistic:                     43.78
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.96e-09
Time:                        04:51:44   Log-Likelihood:                -133.50
No. Observations:                 100   AIC:                             271.0
Df Residuals:                      98   BIC:                             276.2
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.8690      0.193      9.688      0.000       1.486       2.252
x1             2.3952      0.362      6.617      0.000       1.677       3.114
==============================================================================
Omnibus:                        0.538   Durbin-Watson:                   1.940
Prob(Omnibus):                  0.764   Jarque-Bera (JB):                0.650
Skew:                          -0.160   Prob(JB):                        0.723
Kurtosis:                       2.768   Cond. No.                         4.80
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The coefficient value has increased, and the \(R^2\) value has decreased
marginally. We can still reject the null hypothesis based on the p-value.
</p>
</div>
</div>

<div id="outline-container-org51964ec" class="outline-4">
<h4 id="org51964ec">Least squares fit with <code>x2</code> only</h4>
<div class="outline-text-4" id="text-org51964ec">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">coll_model3</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x2"</span>, data=df_coll<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coll_model3.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.206
Model:                            OLS   Adj. R-squared:                  0.198
Method:                 Least Squares   F-statistic:                     25.46
Date:                Sat, 30 May 2020   Prob (F-statistic):           2.08e-06
Time:                        04:51:59   Log-Likelihood:                -140.42
No. Observations:                 100   AIC:                             284.8
Df Residuals:                      98   BIC:                             290.0
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      2.2853      0.171     13.355      0.000       1.946       2.625
x2             3.0393      0.602      5.046      0.000       1.844       4.235
==============================================================================
Omnibus:                        0.036   Durbin-Watson:                   2.117
Prob(Omnibus):                  0.982   Jarque-Bera (JB):                0.064
Skew:                          -0.038   Prob(JB):                        0.969
Kurtosis:                       2.902   Cond. No.                         6.38
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The coefficient value is much larger now, but the \(R^2\) value has decreased.
We can now reject the null hypothesis based on the p-value.
</p>
</div>
</div>

<div id="outline-container-orgcbd16b4" class="outline-4">
<h4 id="orgcbd16b4">Contradiction of models</h4>
<div class="outline-text-4" id="text-orgcbd16b4">
<p>
The three models do not contradict each other. Due to the high correlation
between <code>x1</code> and <code>x2</code>, we can predict <code>x2</code> from <code>x1</code>. Thus in the original model
<code>x2</code> has very little explanatory power and so we cannot reject the null
hypothesis for \(Î²_2\).
</p>

<p>
For the second and third model the explanation for the
increase in the coefficients is as follows. In the second model we are
expressing <code>x2</code> in terms of <code>x1</code>, and so the coefficient of <code>x1</code> in the
expression for <code>y</code> increases to 2.3. In the third model we are expressing <code>x1</code>
in terms of <code>x2</code>, and so the coefficient of <code>x2</code> in the expression for <code>y</code>
increases to 4.3. The 95% confidence interval of the second model includes the
new true value of the coefficient. Even though the 95% confidence interval of
the third model does not include the new true value of the coefficient it comes
close. This is probably due to the difference between the random number
generators used by <code>R</code> and <code>numpy</code>.
</p>
</div>
</div>

<div id="outline-container-org8a053d2" class="outline-4">
<h4 id="org8a053d2">Additional data</h4>
<div class="outline-text-4" id="text-org8a053d2">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">df_coll</span> = df_coll.append<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span><span style="color: #2aa198;">"x1"</span> : 0.1, <span style="color: #2aa198;">"x2"</span> : 0.8, <span style="color: #2aa198;">"y"</span> : 6<span style="color: #d33682;">}</span>, ignore_index=<span style="color: #6c71c4; font-weight: bold;">True</span><span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">coll_model1</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x1 + x2"</span>, data=df_coll<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">coll_model2</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x1"</span>, data=df_coll<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">coll_model3</span> = smf.ols<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"y ~ x2"</span>, data=df_coll<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coll_model1.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.295
Model:                            OLS   Adj. R-squared:                  0.281
Method:                 Least Squares   F-statistic:                     20.50
Date:                Sat, 30 May 2020   Prob (F-statistic):           3.66e-08
Time:                        04:56:57   Log-Likelihood:                -138.91
No. Observations:                 101   AIC:                             283.8
Df Residuals:                      98   BIC:                             291.7
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.9525      0.200      9.769      0.000       1.556       2.349
x1             1.2762      0.508      2.514      0.014       0.269       2.283
x2             2.0004      0.753      2.657      0.009       0.506       3.495
==============================================================================
Omnibus:                        0.020   Durbin-Watson:                   2.024
Prob(Omnibus):                  0.990   Jarque-Bera (JB):                0.144
Skew:                           0.018   Prob(JB):                        0.931
Kurtosis:                       2.819   Cond. No.                         9.92
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coll_model2.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.244
Model:                            OLS   Adj. R-squared:                  0.237
Method:                 Least Squares   F-statistic:                     31.98
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.51e-07
Time:                        04:57:04   Log-Likelihood:                -142.43
No. Observations:                 101   AIC:                             288.9
Df Residuals:                      99   BIC:                             294.1
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      2.0051      0.205      9.786      0.000       1.599       2.412
x1             2.1847      0.386      5.655      0.000       1.418       2.951
==============================================================================
Omnibus:                        6.115   Durbin-Watson:                   1.839
Prob(Omnibus):                  0.047   Jarque-Bera (JB):                7.252
Skew:                           0.308   Prob(JB):                       0.0266
Kurtosis:                       4.160   Cond. No.                         4.76
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coll_model3.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.249
Model:                            OLS   Adj. R-squared:                  0.242
Method:                 Least Squares   F-statistic:                     32.90
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.06e-07
Time:                        04:57:12   Log-Likelihood:                -142.07
No. Observations:                 101   AIC:                             288.1
Df Residuals:                      99   BIC:                             293.4
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      2.2419      0.168     13.365      0.000       1.909       2.575
x2             3.2762      0.571      5.736      0.000       2.143       4.409
==============================================================================
Omnibus:                        0.044   Durbin-Watson:                   2.139
Prob(Omnibus):                  0.978   Jarque-Bera (JB):                0.040
Skew:                          -0.033   Prob(JB):                        0.980
Kurtosis:                       2.927   Cond. No.                         6.09
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

<p>
The \(R^2\) values for models 1 and 2 decreased. This observation decreased the
predictive power of the models. The average leverage for the data set is
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">p</span> = <span style="color: #d33682; font-style: italic;">len</span><span style="color: #268bd2;">(</span>df_coll.columns<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">n</span> = <span style="color: #d33682; font-style: italic;">len</span><span style="color: #268bd2;">(</span>df_coll<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">lev</span> = <span style="color: #268bd2;">(</span>p + 1<span style="color: #268bd2;">)</span> / n
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"{lev:.3f}"</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
0.040

</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">rlplot</span> = sm.graphics.influence_plot<span style="color: #268bd2;">(</span>coll_model1, criterion=<span style="color: #2aa198;">"Cooks"</span>, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Leverage"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
ax.set_title<span style="color: #268bd2;">(</span><span style="color: #2aa198;">" "</span><span style="color: #268bd2;">)</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.14.g_coll1_res_vs_lev.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.14.g_coll1_res_vs_lev.png" alt="3.14.g_coll1_res_vs_lev.png" />
</p>
</div>

<p>
For the first model it is both an outlier and a high leverage point.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">rlplot</span> = sm.graphics.influence_plot<span style="color: #268bd2;">(</span>coll_model2, criterion=<span style="color: #2aa198;">"Cooks"</span>, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Leverage"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
ax.set_title<span style="color: #268bd2;">(</span><span style="color: #2aa198;">" "</span><span style="color: #268bd2;">)</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.14.g_coll2_res_vs_lev.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.14.g_coll12_res_vs_lev.png" alt="3.14.g_coll12_res_vs_lev.png" />
</p>
</div>

<p>
For the second model it is just an outlier.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">rlplot</span> = sm.graphics.influence_plot<span style="color: #268bd2;">(</span>coll_model3, criterion=<span style="color: #2aa198;">"Cooks"</span>, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
ax.set_xlabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Leverage"</span><span style="color: #268bd2;">)</span>
ax.set_ylabel<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Standardized Residuals"</span><span style="color: #268bd2;">)</span>
ax.set_title<span style="color: #268bd2;">(</span><span style="color: #2aa198;">" "</span><span style="color: #268bd2;">)</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.14.g_coll3_res_vs_lev.png"</span>, dpi=90<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.14.g_coll3_res_vs_lev.png" alt="3.14.g_coll3_res_vs_lev.png" />
</p>
</div>

<p>
For the third model it is just an high leverage point.
</p>
</div>
</div>
</div>
<div id="outline-container-org2062a94" class="outline-3">
<h3 id="org2062a94">Question 15</h3>
<div class="outline-text-3" id="text-org2062a94">
</div>
<div id="outline-container-org22249f6" class="outline-4">
<h4 id="org22249f6">Predict per capita crime rate with the <code>Boston</code> data set</h4>
<div class="outline-text-4" id="text-org22249f6">
<p>
We load the <code>Boston</code> from <code>statsmodels</code>.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.api <span style="color: #859900; font-weight: bold;">as</span> sm

<span style="color: #268bd2;">boston</span> = sm.datasets.get_rdataset<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"Boston"</span>, <span style="color: #2aa198;">"MASS"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>boston.<span style="color: #d33682; font-style: italic;">__doc__</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
+--------+-----------------+
| Boston | R Documentation |
+--------+-----------------+

Housing Values in Suburbs of Boston
-----------------------------------

Description
~~~~~~~~~~~

The ``Boston`` data frame has 506 rows and 14 columns.

Usage
~~~~~

::

   Boston

Format
~~~~~~

This data frame contains the following columns:

``crim``
   per capita crime rate by town.

``zn``
   proportion of residential land zoned for lots over 25,000 sq.ft.

``indus``
   proportion of non-retail business acres per town.

``chas``
   Charles River dummy variable (= 1 if tract bounds river; 0
   otherwise).

``nox``
   nitrogen oxides concentration (parts per 10 million).

``rm``
   average number of rooms per dwelling.

``age``
   proportion of owner-occupied units built prior to 1940.

``dis``
   weighted mean of distances to five Boston employment centres.

``rad``
   index of accessibility to radial highways.

``tax``
   full-value property-tax rate per \\$10,000.

``ptratio``
   pupil-teacher ratio by town.

``black``
   *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.

``lstat``
   lower status of the population (percent).

``medv``
   median value of owner-occupied homes in \\$1000s.

Source
~~~~~~

Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand
for clean air. *J. Environ. Economics and Management* **5**, 81â102.

Belsley D.A., Kuh, E. and Welsch, R.E. (1980) *Regression Diagnostics.
Identifying Influential Data and Sources of Collinearity.* New York:
Wiley.
</pre>

<p>
Earlier we are fitted <code>medv</code> to the other predictors, now we will be fitting
<code>crim</code> to the other predictors.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> statsmodels.formula.api <span style="color: #859900; font-weight: bold;">as</span> smf

<span style="color: #268bd2;">df</span> = boston.data
<span style="color: #268bd2;">predictors</span> = <span style="color: #268bd2;">[</span>c <span style="color: #859900; font-weight: bold;">for</span> c <span style="color: #859900; font-weight: bold;">in</span> df.columns <span style="color: #859900; font-weight: bold;">if</span> c != <span style="color: #2aa198;">"crim"</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">simple_models</span> = <span style="color: #268bd2;">{</span>p : smf.ols<span style="color: #d33682;">(</span>formula=f<span style="color: #2aa198;">"crim ~ {p}"</span>, data=df<span style="color: #d33682;">)</span>.fit<span style="color: #d33682;">()</span> <span style="color: #859900; font-weight: bold;">for</span> p <span style="color: #859900; font-weight: bold;">in</span> predictors<span style="color: #268bd2;">}</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"predictor coefficient p-value"</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">for</span> p, model <span style="color: #859900; font-weight: bold;">in</span> simple_models.items<span style="color: #268bd2;">()</span>:
    <span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"{p:^9} {model.params[p]:&gt;9,.4f} {model.pvalues[p]:&gt;9,.4f}"</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
predictor coefficient p-value
   zn       -0.0739    0.0000
  indus      0.5098    0.0000
  chas      -1.8928    0.2094
   nox      31.2485    0.0000
   rm       -2.6841    0.0000
   age       0.1078    0.0000
   dis      -1.5509    0.0000
   rad       0.6179    0.0000
   tax       0.0297    0.0000
 ptratio     1.1520    0.0000
  black     -0.0363    0.0000
  lstat      0.5488    0.0000
  medv      -0.3632    0.0000
</pre>

<p>
Except for <code>chas</code> everything else appears to be statistically significant.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #859900; font-weight: bold;">as</span> plt
<span style="color: #859900; font-weight: bold;">import</span> seaborn <span style="color: #859900; font-weight: bold;">as</span> sns
<span style="color: #859900; font-weight: bold;">from</span> math <span style="color: #859900; font-weight: bold;">import</span> ceil

sns.set_style<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"ticks"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">ncols</span> = 4
<span style="color: #268bd2;">nrows</span> = ceil<span style="color: #268bd2;">(</span><span style="color: #d33682; font-style: italic;">len</span><span style="color: #d33682;">(</span>predictors<span style="color: #d33682;">)</span> / ncols<span style="color: #268bd2;">)</span>

plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">axs</span> = plt.subplots<span style="color: #268bd2;">(</span>nrows=nrows, ncols=ncols, constrained_layout=<span style="color: #6c71c4; font-weight: bold;">True</span>, figsize=<span style="color: #d33682;">(</span>12, 10<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">for</span> i <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #d33682; font-style: italic;">range</span><span style="color: #268bd2;">(</span>nrows<span style="color: #268bd2;">)</span>:
    <span style="color: #859900; font-weight: bold;">for</span> j <span style="color: #859900; font-weight: bold;">in</span> <span style="color: #d33682; font-style: italic;">range</span><span style="color: #268bd2;">(</span>ncols<span style="color: #268bd2;">)</span>:
        <span style="color: #859900; font-weight: bold;">if</span> i * ncols + j &lt; <span style="color: #d33682; font-style: italic;">len</span><span style="color: #268bd2;">(</span>predictors<span style="color: #268bd2;">)</span>:
            sns.regplot<span style="color: #268bd2;">(</span>x=df<span style="color: #d33682;">[</span>predictors<span style="color: #859900;">[</span>i * ncols + j<span style="color: #859900;">]</span><span style="color: #d33682;">]</span>, y=df<span style="color: #d33682;">[</span><span style="color: #2aa198;">"crim"</span><span style="color: #d33682;">]</span>, ax=axs<span style="color: #d33682;">[</span>i, j<span style="color: #d33682;">]</span>, line_kws=<span style="color: #d33682;">{</span><span style="color: #2aa198;">"color"</span> : <span style="color: #2aa198;">"r"</span><span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>
            sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.15.a_reg_mat.png"</span>, dpi=120<span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.15.a_reg_mat.png" alt="3.15.a_reg_mat.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org20e3b7c" class="outline-4">
<h4 id="org20e3b7c">Multiple regression with <code>Boston</code> data set</h4>
<div class="outline-text-4" id="text-org20e3b7c">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #268bd2;">Y</span> = df<span style="color: #268bd2;">[</span><span style="color: #2aa198;">'crim'</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">X</span> = df<span style="color: #268bd2;">[</span>predictors<span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">X</span> = sm.add_constant<span style="color: #268bd2;">(</span>X<span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">ml_model</span> = sm.OLS<span style="color: #268bd2;">(</span>Y, X<span style="color: #268bd2;">)</span>.fit<span style="color: #268bd2;">()</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>ml_model.summary<span style="color: #d33682;">()</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
                            OLS Regression Results
==============================================================================
Dep. Variable:                   crim   R-squared:                       0.454
Model:                            OLS   Adj. R-squared:                  0.440
Method:                 Least Squares   F-statistic:                     31.47
Date:                Sat, 30 May 2020   Prob (F-statistic):           1.57e-56
Time:                        12:26:02   Log-Likelihood:                -1653.3
No. Observations:                 506   AIC:                             3335.
Df Residuals:                     492   BIC:                             3394.
Df Model:                          13
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         17.0332      7.235      2.354      0.019       2.818      31.248
zn             0.0449      0.019      2.394      0.017       0.008       0.082
indus         -0.0639      0.083     -0.766      0.444      -0.228       0.100
chas          -0.7491      1.180     -0.635      0.526      -3.068       1.570
nox          -10.3135      5.276     -1.955      0.051     -20.679       0.052
rm             0.4301      0.613      0.702      0.483      -0.774       1.634
age            0.0015      0.018      0.081      0.935      -0.034       0.037
dis           -0.9872      0.282     -3.503      0.001      -1.541      -0.433
rad            0.5882      0.088      6.680      0.000       0.415       0.761
tax           -0.0038      0.005     -0.733      0.464      -0.014       0.006
ptratio       -0.2711      0.186     -1.454      0.147      -0.637       0.095
black         -0.0075      0.004     -2.052      0.041      -0.015      -0.000
lstat          0.1262      0.076      1.667      0.096      -0.023       0.275
medv          -0.1989      0.061     -3.287      0.001      -0.318      -0.080
==============================================================================
Omnibus:                      666.613   Durbin-Watson:                   1.519
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            84887.625
Skew:                           6.617   Prob(JB):                         0.00
Kurtosis:                      65.058   Cond. No.                     1.58e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.58e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

<p>
Based on the p-values we can reject the null hypothesis for <code>dis</code>, <code>rad</code>, and
<code>medv</code>. If we are willing to be less accurate we can also reject the null
hypothesis for <code>zn</code>, <code>nox</code>, <code>black</code>, and <code>lstat</code>.
</p>
</div>
</div>

<div id="outline-container-orgb658cac" class="outline-4">
<h4 id="orgb658cac">Comparison plot</h4>
<div class="outline-text-4" id="text-orgb658cac">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">import</span> pandas <span style="color: #859900; font-weight: bold;">as</span> pd

<span style="color: #268bd2;">ml_coefs</span> = ml_model.params
<span style="color: #268bd2;">sl_coefs</span> = pd.Series<span style="color: #268bd2;">(</span><span style="color: #d33682;">{</span>p : simple_models<span style="color: #859900;">[</span>p<span style="color: #859900;">]</span>.params.loc<span style="color: #859900;">[</span>p<span style="color: #859900;">]</span> <span style="color: #859900; font-weight: bold;">for</span> p <span style="color: #859900; font-weight: bold;">in</span> predictors<span style="color: #d33682;">}</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">coef_df</span> = pd.concat<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>sl_coefs, ml_coefs<span style="color: #d33682;">]</span>, axis=1<span style="color: #268bd2;">)</span>
coef_df.reset_index<span style="color: #268bd2;">(</span>inplace=<span style="color: #6c71c4; font-weight: bold;">True</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">coef_df.columns</span> = <span style="color: #268bd2;">[</span><span style="color: #2aa198;">"Predictors"</span>, <span style="color: #2aa198;">"Simple OLS Coefficient"</span>, <span style="color: #2aa198;">"Multiple OLS Coefficient"</span><span style="color: #268bd2;">]</span>
coef_df.dropna<span style="color: #268bd2;">(</span>inplace=<span style="color: #6c71c4; font-weight: bold;">True</span><span style="color: #268bd2;">)</span>
<span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>coef_df<span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
   Predictors  Simple OLS Coefficient  Multiple OLS Coefficient
0          zn               -0.073935                  0.044855
1       indus                0.509776                 -0.063855
2        chas               -1.892777                 -0.749134
3         nox               31.248531                -10.313535
4          rm               -2.684051                  0.430131
5         age                0.107786                  0.001452
6         dis               -1.550902                 -0.987176
7         rad                0.617911                  0.588209
8         tax                0.029742                 -0.003780
9     ptratio                1.151983                 -0.271081
10      black               -0.036280                 -0.007538
11      lstat                0.548805                  0.126211
12       medv               -0.363160                 -0.198887
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python">plt.close<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"all"</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">fig</span>, <span style="color: #268bd2;">ax</span> = plt.subplots<span style="color: #268bd2;">()</span>
sns.scatterplot<span style="color: #268bd2;">(</span>x=<span style="color: #2aa198;">"Simple OLS Coefficient"</span>, y=<span style="color: #2aa198;">"Multiple OLS Coefficient"</span>, data=coef_df, ax=ax<span style="color: #268bd2;">)</span>
sns.despine<span style="color: #268bd2;">()</span>
fig.savefig<span style="color: #268bd2;">(</span><span style="color: #2aa198;">"img/3.15.c_comp_plot.png"</span><span style="color: #268bd2;">)</span>
</pre>
</div>


<div class="figure">
<p><img src="img/3.15.c_comp_plot.png" alt="3.15.c_comp_plot.png" />
</p>
</div>

<p>
The coefficients for <code>nox</code> are very different in the two models.
</p>
</div>
</div>

<div id="outline-container-org93bccb1" class="outline-4">
<h4 id="org93bccb1">Evidence of non-linear associations</h4>
<div class="outline-text-4" id="text-org93bccb1">
<p>
We will use <code>scikit-learn</code> to generate the non-linear features.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900; font-weight: bold;">from</span> sklearn.preprocessing <span style="color: #859900; font-weight: bold;">import</span> PolynomialFeatures
<span style="color: #268bd2;">pd.options.display.float_format</span> = <span style="color: #2aa198;">"{:,.3f}"</span>.<span style="color: #d33682; font-style: italic;">format</span>

<span style="color: #268bd2;">Y</span> = df<span style="color: #268bd2;">[</span><span style="color: #2aa198;">'crim'</span><span style="color: #268bd2;">]</span>
<span style="color: #268bd2;">poly_features</span> = PolynomialFeatures<span style="color: #268bd2;">(</span>degree=3<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">poly_predictors</span> = <span style="color: #268bd2;">{</span>p : poly_features.fit_transform<span style="color: #d33682;">(</span>df<span style="color: #859900;">[</span>p<span style="color: #859900;">][</span>:, <span style="color: #6c71c4; font-weight: bold;">None</span><span style="color: #859900;">]</span><span style="color: #d33682;">)</span> <span style="color: #859900; font-weight: bold;">for</span> p <span style="color: #859900; font-weight: bold;">in</span> predictors<span style="color: #268bd2;">}</span>
<span style="color: #268bd2;">poly_models</span> = <span style="color: #268bd2;">{</span>p : sm.OLS<span style="color: #d33682;">(</span>Y, poly_predictors<span style="color: #859900;">[</span>p<span style="color: #859900;">]</span><span style="color: #d33682;">)</span>.fit<span style="color: #d33682;">()</span> <span style="color: #859900; font-weight: bold;">for</span> p <span style="color: #859900; font-weight: bold;">in</span> predictors<span style="color: #268bd2;">}</span>
<span style="color: #859900; font-weight: bold;">for</span> p <span style="color: #859900; font-weight: bold;">in</span> predictors:
    <span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"p-values for {p}:"</span><span style="color: #268bd2;">)</span>
    <span style="color: #859900; font-weight: bold;">print</span><span style="color: #268bd2;">(</span>f<span style="color: #2aa198;">"{poly_models[p].pvalues}\n"</span><span style="color: #268bd2;">)</span>
</pre>
</div>

<pre class="example">
p-values for zn:
const   0.000
x1      0.003
x2      0.094
x3      0.230
dtype: float64

p-values for indus:
const   0.020
x1      0.000
x2      0.000
x3      0.000
dtype: float64

p-values for chas:
const   0.000
x1      0.209
x2      0.209
x3      0.209
dtype: float64

p-values for nox:
const   0.000
x1      0.000
x2      0.000
x3      0.000
dtype: float64

p-values for rm:
const   0.081
x1      0.212
x2      0.364
x3      0.509
dtype: float64

p-values for age:
const   0.358
x1      0.143
x2      0.047
x3      0.007
dtype: float64

p-values for dis:
const   0.000
x1      0.000
x2      0.000
x3      0.000
dtype: float64

p-values for rad:
const   0.768
x1      0.623
x2      0.613
x3      0.482
dtype: float64

p-values for tax:
const   0.105
x1      0.110
x2      0.137
x3      0.244
dtype: float64

p-values for ptratio:
const   0.002
x1      0.003
x2      0.004
x3      0.006
dtype: float64

p-values for black:
const   0.000
x1      0.139
x2      0.474
x3      0.544
dtype: float64

p-values for lstat:
const   0.554
x1      0.335
x2      0.065
x3      0.130
dtype: float64

p-values for medv:
const   0.000
x1      0.000
x2      0.000
x3      0.000
dtype: float64
</pre>

<p>
From the p-values we see that there is evidence of polynomial association
between the response and the predictors <code>indus</code>, <code>nox</code>, <code>age</code>, <code>dis</code>, <code>ptratio</code>,
and <code>medv</code>.
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
